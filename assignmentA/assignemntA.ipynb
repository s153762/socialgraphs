{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import collections\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from fa2 import ForceAtlas2\n",
    "# conda install -c phlya adjusttext \n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 wikis found in Games\n",
      "250 wikis found in Other\n",
      "30 wikis found in Lifestyle\n",
      "250 wikis found in Books\n",
      "250 wikis found in TV\n",
      "250 wikis found in Comics\n",
      "250 wikis found in Movies\n",
      "250 wikis found in Music\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "n = 2500\n",
    "\n",
    "# Initial query - get most active wikis\n",
    "query = \"http://www.wikia.com/api/v1/Wikis/List?expand=1&limit=\"+str(n)+\"&batch=1\"\n",
    "response = urlopen(query)\n",
    "wikisource = response.read()\n",
    "data = json.loads(wikisource)\n",
    "df = pd.DataFrame(data['items'])\n",
    "fullDataSet = df\n",
    " \n",
    "# Search for more wikis (based on the different hub names)\n",
    "def searchForWiki(hub, n):\n",
    "    querySearch = \"http://www.wikia.com/api/v1/Wikis/ByString?expand=1&string=\"+str(hub)+\"&limit=\"+str(n)+\"&batch=1&includeDomain=true\"\n",
    "    response = urlopen(querySearch)\n",
    "    data = json.loads(response.read())\n",
    "    return pd.DataFrame(data['items']) \n",
    "    \n",
    "# use hubs in dataset to searsh for wikis\n",
    "for hub in set(df[\"hub\"]):\n",
    "    df2 = searchForWiki(hub, n)\n",
    "    fullDataSet = fullDataSet.append(df2)\n",
    "    print(\"%s wikis found in %s\" % (str(len(df2)), hub))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "2030\n"
     ]
    }
   ],
   "source": [
    "# remove inactive wikis\n",
    "def findActiveWikis(dataSet):\n",
    "    activeDataSet = pd.DataFrame()\n",
    "    for s in dataSet['stats']:\n",
    "        data = dataSet.loc[dataSet['stats']==s]\n",
    "        if s['activeUsers']>1:\n",
    "            activeDataSet = activeDataSet.append(data)\n",
    "    return activeDataSet\n",
    "\n",
    "activeDataSet = findActiveWikis(fullDataSet)\n",
    "print(len(activeDataSet))\n",
    "print(len(fullDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "print(len(activeDataSet))\n",
    "activeDataSet = activeDataSet.drop_duplicates(subset=\"id\")\n",
    "activeDataSet = activeDataSet.reset_index()\n",
    "print(len(activeDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findUsernames(userID):\n",
    "    querySearch = \"http://community.wikia.com/wiki/Special:Search?search=\"+userID+\"&fulltext=Search&ns2=1\"\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    usernames = set(re.findall(\"User:(\\w+)?\",str(wikisource)))\n",
    "    return usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWikisThroughUser(usernames):\n",
    "    wikis = []\n",
    "    for username in usernames:\n",
    "        querySearch = \"http://community.wikia.com/index.php?limit=1000&tagfilter=&title=Special%3AContributions&target=\"+username+\"&namespace=&tagfilter=&year=&month=-1\"\n",
    "        response = urlopen(querySearch)\n",
    "        wikisource = response.read()\n",
    "        wikis.append(set(re.findall(\"Adoption:(\\w+)?\",str(wikisource))))\n",
    "    return wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addUserToWiki(wiki,userID, dataSet):\n",
    "    series = dataSet.loc[dataSet['name'] == wiki]\n",
    "    topUsers = list(series['topUsers'].values)\n",
    "    topUsers = list(itertools.chain.from_iterable(topUsers))\n",
    "    if userID not in topUsers:\n",
    "        topUsers.append(int(userID))\n",
    "        dataSet.at[series.index.values[0], \"topUsers\"] = set(topUsers)\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printUserInWiki(userID, dataSet):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for users in list(dataSet['topUsers'].values):\n",
    "        if int(userID) in list(users):\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "    #print(\"%s user found in %s wikis, not in %s wikis\" % (userID,i,j))  \n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWikis(userID, dataSet):\n",
    "    wikiList = findWikisThroughUser(findUsernames(userID))\n",
    "    i = printUserInWiki(userID, dataSet)\n",
    "    \n",
    "    for wikis in list(wikiList):\n",
    "        for wiki in set(list(wikis)):\n",
    "            wiki = wiki.replace(\"_\", \" \")\n",
    "            if wiki in list(dataSet['name']):\n",
    "                #print(\"%s found in Dataset\" % wiki)\n",
    "                dataSet = addUserToWiki(wiki,userID, dataSet)\n",
    "            else:\n",
    "                wiki = wiki.replace(\" \", \"_\")\n",
    "                wikidf = searchForWiki(wiki, 250)\n",
    "                # wikidf = findActiveWikis(wikidf)\n",
    "                dataSet = dataSet.append(wikidf)\n",
    "                dataSet = dataSet.drop_duplicates(subset=\"name\")\n",
    "                dataSet = dataSet.reset_index(drop=True)\n",
    "                wiki = wiki.replace(\"_\", \" \")\n",
    "                if wiki in dataSet['name']:\n",
    "                    dataSet = addUserToWiki(wiki,userID, dataSet)\n",
    "                    print(\"wiki %s added to Dataset and user added\" % wiki)\n",
    "                #else:\n",
    "                    #print(\"--- %s not found in Dataset\" % wiki)\n",
    "    \n",
    "    #j = printUserInWiki(userID, fullDataSet)\n",
    "    #print(\"User added to %s new wikis\" % j-i)\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 af 2039, wiki count = 259\n"
     ]
    }
   ],
   "source": [
    "# make graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# find topusers\n",
    "#topUsers = []\n",
    "topUsers = [x for x in activeDataSet[\"topUsers\"]]\n",
    "topUsers = list(itertools.chain.from_iterable(topUsers))\n",
    "for i in range(len(topUsers)):\n",
    "    activeDataSet = addWikis(str(topUsers[i]), activeDataSet)\n",
    "    activeDataSet = findActiveWikis(activeDataSet)\n",
    "    if(i%100==0):\n",
    "        print(\"%s af %s, wiki count = %s\" % (str(i),str(len(topUsers)),str(len(activeDataSet))))\n",
    "        \n",
    "topUsers = [x for x in activeDataSet[\"topUsers\"]]\n",
    "topUsers = list(itertools.chain.from_iterable(topUsers))\n",
    "\n",
    "counter=collections.Counter(topUsers)\n",
    "#print(counter)\n",
    "#print(counter.values())\n",
    "#print(counter.keys())\n",
    "print(counter.most_common(7))\n",
    "\n",
    "# add nodes\n",
    "for wiki in activeDataSet[\"name\"]:\n",
    "    G.add_node(wiki, hub=set(activeDataSet['hub'].loc[activeDataSet['name'] ==wiki].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topUsers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the wiki names of the users\n",
    "users = {}\n",
    "for user in counter.most_common():\n",
    "    user = user[0]\n",
    "    for k, v in df.T.items():\n",
    "        if user in v[\"topUsers\"]:\n",
    "            if user not in users:\n",
    "                users[user] = []\n",
    "            users[user].append(v['name'])\n",
    "\n",
    "# create edges between all wikis which share users\n",
    "def createEdges(G,userWikis):\n",
    "    length = len(userWikis)\n",
    "    if length==1:\n",
    "        return G\n",
    "    elif length>1:\n",
    "        wiki = userWikis.pop()\n",
    "        for w in userWikis:\n",
    "            if G.has_edge(wiki, w):\n",
    "                G[wiki][w]['weight']+=1\n",
    "            else:\n",
    "                G.add_edge(wiki,w,weight=1)\n",
    "        return createEdges(G,userWikis)\n",
    "    else:\n",
    "        return G\n",
    "\n",
    "# iterate over all users\n",
    "for k,v in set(users.items()):\n",
    "    print(set(v))\n",
    "    G = createEdges(G,set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                          # Behavior alternatives\n",
    "                          outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                          linLogMode=False,  # NOT IMPLEMENTED\n",
    "                          adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                          edgeWeightInfluence=1,\n",
    "\n",
    "                          # Performance\n",
    "                          jitterTolerance=0.5,  #1 # Tolerance\n",
    "                          barnesHutOptimize=True,\n",
    "                          barnesHutTheta=1.2,\n",
    "                          multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                          # Tuning\n",
    "                          scalingRatio=0.01, #0.01\n",
    "                          strongGravityMode=False, #False\n",
    "                          gravity=10, #15\n",
    "\n",
    "                          # Log\n",
    "                          verbose=True)\n",
    "\n",
    "\n",
    "# Calculate Positions\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot figure\n",
    "plt.figure(figsize=(18, 18))             \n",
    "node_size = [(G.degree(node, weight='weight')*20) for node in G.nodes()]  \n",
    "\n",
    "labels = {}    \n",
    "for node in G.nodes():\n",
    "    if G.degree(node, weight='weight')*20 > 80:\n",
    "        #set the node name as the key and the label as its value \n",
    "        labels[node] = node\n",
    "\n",
    "node_label = [node for node in G.nodes() ]\n",
    "nx.draw_networkx_nodes(G, positions, nodelist=G.nodes, node_size=node_size,cmap=plt.get_cmap('jet'))\n",
    "nx.draw_networkx_edges(G, positions, width=0.5, cmap=plt.get_cmap('jet'))\n",
    "nx.draw_networkx_labels(G, positions, labels=labels, font_size=18, font_color='k', font_weight='normal', alpha=2.0)\n",
    "#texts = [plt.text(n, labels[n], ha='center', va='center') for n in G.nodes()]\n",
    "#adjust_text(texts)#, only_move='y', arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
