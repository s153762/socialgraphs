{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate Fandoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Motivation\n",
    "\n",
    "Almost all people are interested in pop culture to some extent - some more than others of course. With the spread of the Internet in the 00’s, fans of the same books, movies, games, or music, etc. were given the opportunity to share and cultivate experiences about their common interests with each other. \n",
    "Furthermore, this also gives the opportunity to investigate and clarify the interaction across the different communities. This can be done by downloading data from these platforms, and then creating and analyzing social graphs from the data. And this is exactly what we intend to do.\n",
    "\n",
    "We will use fandom.com, which is wiki hosting service where roughly all fandoms have a wiki. From the Fandom API we can collect data concerning the top contributing users for every fandom-wiki, how big the fanbase is and how the community interact with each other in the forum.\n",
    "\n",
    "* What is your dataset?\n",
    "\n",
    "Our dataset consist of X fandoms, and Y users who contribute to the fandoms.\n",
    "In addition, we have included multiple articles for the different wiki in order to better understand or the use and overall sentiment. \n",
    "\n",
    "* Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "The data is interesting, as it is very broad \n",
    "\n",
    "* What was your goal for the end user's experience?\n",
    "\n",
    "The final goal is to understand which fandoms people are pationated about, as well as find the connection between fandoms. What does the typical gamer have of other interest? Are people how are active in the gaming communities also interested in other topics, or is the typical fan only contained to one or two different areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the whole Analysis\n",
    "\n",
    "#from urllib.request import urlopen\n",
    "import json\n",
    "import re\n",
    "#from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import collections\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from fa2 import ForceAtlas2\n",
    "import numpy as np\n",
    "import datetime\n",
    "import ast # string to dirc\n",
    "import time\n",
    "import community as cm\n",
    "import matplotlib.cm as pltcm\n",
    "import glob\n",
    "\n",
    "# imports for text-analysis\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# conda/pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "# conda install -c phlya adjusttext \n",
    "#from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Basic stats\n",
    "\n",
    "\n",
    "* Write about your choices in data cleaning and preprocessing.\n",
    "* Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get wikis/dataset from csv\n",
    "fullDataSet = pd.read_csv(\"data/sortedWikiData.csv\", sep='\\t')\n",
    "\n",
    "# Drop old index column named \"Unnamed: 0\"\n",
    "fullDataSet = fullDataSet.drop(['Unnamed: 0'], axis=1)\n",
    "fullDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methode that remove inactive wikis and duplicates\n",
    "def findActiveWikis(dataSet):\n",
    "    activeDataSet = pd.DataFrame()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    dataSet = dataSet.drop_duplicates()\n",
    "    \n",
    "    # Find stats in dataset\n",
    "    for wiki in dataSet.T.items():\n",
    "        wiki = wiki[1]\n",
    "        stats = wiki['stats']\n",
    "        \n",
    "        # Only add wiki to list when at in contains at least 1 active user\n",
    "        sDirc = ast.literal_eval(stats)\n",
    "        if int(sDirc['activeUsers'])>1:\n",
    "            activeDataSet = activeDataSet.append(wiki) \n",
    "    \n",
    "    # Create new index\n",
    "    activeDataSet = activeDataSet.reset_index(drop=True)\n",
    "    return activeDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter Wiki Data ###\n",
    "activeDataSet = findActiveWikis(fullDataSet)\n",
    "\n",
    "print(\"%s Wikis before filter, and %s Wikis efter filter.\\n%s has been removed\" % \n",
    "      (len(fullDataSet),len(activeDataSet),len(fullDataSet)-len(activeDataSet)))\n",
    "\n",
    "activeDataSet.head()\n",
    "\n",
    "# Data memory usage\n",
    "print(sum(activeDataSet.memory_usage(index=False, deep=True))/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikis will be the nodes of our network, however we need to connect these nodes. We will do this by finding multiple user profiles and connecting all the wikis which the individual user is active in. The users will thereby create small cliques, that in the end will give a knowledge over which wikis are the most connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methode that tage a string representing a list and makes it into a list\n",
    "def makeStringToList(string):\n",
    "    if string == \"[]\":\n",
    "        return []\n",
    "    string = string.replace(\"['\",\"\")\n",
    "    string = string.replace(\"']\",\"\")\n",
    "    \n",
    "    # Some Wikis have '_' instead og ' ', which needs to be replaced. \n",
    "    string = string.replace(\"_\",\" \")\n",
    "    listCreated = string.split(\"', '\")\n",
    "    return listCreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Users from user-wikis ###\n",
    "users = {}\n",
    "\n",
    "# Open file\n",
    "filename = \"data/users/user-wikis.txt\"\n",
    "f = open(filename, \"r\")\n",
    "\n",
    "# Read content and create dircetory of wikis the users are active in\n",
    "for line in f.read().splitlines():\n",
    "    user = line.split(': ')\n",
    "    if len(user) == 2:\n",
    "        if user[0] not in users:\n",
    "            users[user[0]] = []\n",
    "            \n",
    "        # Remove duplicates and add to the list\n",
    "        users[user[0]] = list(set(users[user[0]]+makeStringToList(user[1])))\n",
    "\n",
    "print(\"%s users have been found in user-wikis.txt\" % len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network\n",
    "\n",
    "The network is going to be a undirected graph containing multilple cliques. The nodes will be the wikis with the attributes: Hub, allUsers and activeUsers according to the dataset, and usersOccurances, which is the number of times the wiki is mentioned in our user data: user-wikis.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make graph\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes and attributes\n",
    "duplicate_name_node_count = 0\n",
    "for wiki in activeDataSet.T.items():\n",
    "    wiki = wiki[1]\n",
    "    name = wiki['name']\n",
    "    \n",
    "    # If wiki is NaN do not create node\n",
    "    if str(name) == \"nan\":\n",
    "        print(\"Do not add wiki if it is NaN (%s)\" % name)\n",
    "        continue\n",
    "    \n",
    "    # Make name fit wiki name in user-wikis.txt\n",
    "    name = name.replace(\"_\",\" \")\n",
    "\n",
    "    # Find hub value\n",
    "    hub = wiki['hub']\n",
    "    \n",
    "    # Find topic values\n",
    "    topic = ''\n",
    "    if str(wiki['topic']) != 'nan':\n",
    "        topic = wiki['topic']\n",
    "    \n",
    "    # Find user values\n",
    "    stats = ast.literal_eval(wiki['stats'])\n",
    "    allUsers = stats['users']\n",
    "    activeUsersStats = stats['activeUsers']\n",
    "    \n",
    "    # Count occurances in user-wikis.txt\n",
    "    usersOccurances = 0\n",
    "    for values in users.values():\n",
    "        for value in values:\n",
    "            if value == name:\n",
    "                usersOccurances+=1\n",
    "    \n",
    "    # Check if Wiki Name have a node\n",
    "    if name not in G.nodes():\n",
    "        # Create new node\n",
    "        G.add_node(name, \n",
    "                   hub=hub, \n",
    "                   topic=topic,\n",
    "                   allUsers=allUsers, \n",
    "                   activeUsers=activeUsersStats, \n",
    "                   usersOccurances=usersOccurances)\n",
    "    else:\n",
    "        # Update excisting node by merging new values\n",
    "        duplicate_name_node_count+=1\n",
    "        if hub not in G.node[name]['hub']:\n",
    "            G.node[name]['hub'] = G.node[name]['hub']+\", \"+hub\n",
    "        \n",
    "        if topic not in G.node[name]['topic']:\n",
    "            G.node[name]['topic'] = G.node[name]['topic']+\", \"+topic\n",
    "        \n",
    "        # Find average of the user numbers \n",
    "        # taking into account if one user is member of both Wikis\n",
    "        if G.node[name]['allUsers'] != allUsers:\n",
    "            G.node[name]['allUsers'] = str(int((int(G.node[name]['allUsers'])+int(allUsers))/2))  \n",
    "                                                                                                                                \n",
    "        if G.node[name]['activeUsers'] != activeUsersStats:\n",
    "            G.node[name]['activeUsers'] = str(int((int(G.node[name]['activeUsers'])+int(activeUsersStats))/2))  \n",
    "        \n",
    "        # If different number of occurances (should not happend), then return the sum\n",
    "        if G.node[name]['usersOccurances'] != usersOccurances:\n",
    "            G.node[name]['usersOccurances'] = str(int(G.node[name]['usersOccurances'])+int(usersOccurances)) \n",
    "\n",
    "print(\"Number of nodes added to the graph: %s,\\nNumber of nodes merged: %s,\\nNumber of wikis: %s\" % \n",
    "      (duplicate_name_node_count, len(G.nodes()), len(activeDataSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methodes that create edges between all wikis which share users (cliques)\n",
    "def createEdges(G,userWikis):\n",
    "    length = len(userWikis)\n",
    "    \n",
    "    # Check if it is the last wiki in the list\n",
    "    if length<=1:\n",
    "        return G\n",
    "    \n",
    "    # Resursive call\n",
    "    elif length>1:\n",
    "        wiki = userWikis.pop()\n",
    "        # Check wiki exsist in dataset\n",
    "        if wiki in G.nodes():\n",
    "             wikisNotInDataSet = []\n",
    "\n",
    "            # Create edges\n",
    "            for w in userWikis:\n",
    "                if w in G.nodes():\n",
    "                    if G.has_edge(wiki, w):\n",
    "                        G[wiki][w]['weight']+=1\n",
    "                    else:\n",
    "                        G.add_edge(wiki,w,weight=1)\n",
    "                else:\n",
    "                    wikisNotInDataSet.append(w) \n",
    "            # Remove wikis which are not a part of our dataset\n",
    "            if len(wikisNotInDataSet) > 0:\n",
    "                length = len(userWikis) \n",
    "                userWikis = list(set(userWikis) - set(wikisNotInDataSet))\n",
    "        return createEdges(G,userWikis)\n",
    "    else:\n",
    "        # Return final grph with edges\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all users\n",
    "for k,v in users.items():\n",
    "    # Create cliques between wiki nodes when user follows them\n",
    "    G = createEdges(G,set(v))\n",
    "\n",
    "# Discover number of edges\n",
    "print(\"Number of edges added to the graph: %s\" % len(G.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw network in graph colored by the hub type\n",
    "The graph contains the nodes and edges. For one to understand the graph it is interesting to draw the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                          # Behavior alternatives\n",
    "                          outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                          linLogMode=False,  # NOT IMPLEMENTED\n",
    "                          adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                          edgeWeightInfluence=1,\n",
    "\n",
    "                          # Performance\n",
    "                          jitterTolerance=0.5,  #1 # Tolerance\n",
    "                          barnesHutOptimize=True,\n",
    "                          barnesHutTheta=1.2,\n",
    "                          multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                          # Tuning\n",
    "                          scalingRatio=0.01, #0.01\n",
    "                          strongGravityMode=False, #False\n",
    "                          gravity=30, #15\n",
    "\n",
    "                          # Log\n",
    "                          verbose=True)\n",
    "\n",
    "\n",
    "# Calculate Positions\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide wikis according to hub\n",
    "hubs = {}\n",
    "for n in G.nodes(): \n",
    "    # Set the node type to only be the initial hub mentioned\n",
    "    nodehub = G.node[n]['hub'].split(', ')[0]\n",
    "    \n",
    "    # Add hub to dirc\n",
    "    if nodehub not in hubs:\n",
    "        hubs[nodehub] = []\n",
    "    hubs[nodehub].append(n)\n",
    "\n",
    "# Print result\n",
    "summ = 0\n",
    "for k, v in hubs.items():\n",
    "    summ += len(v)\n",
    "    print(\"The hub %s has %s wikis from this dataSet\" % (k, len(v)))\n",
    "\n",
    "print(\"The sum of all are %s, which is the same as the %s number of nodes\" % (summ,len(G.nodes())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph can be drawn in multiple different ways. To make it easy the node-size-calculations have their own method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node size according to frequency\n",
    "def nodeSizeFrequency(nodeList):\n",
    "    size = []\n",
    "    for i in range(len(nodeList)):\n",
    "        if str(nodeList[i]) == \"nan\":\n",
    "            continue\n",
    "        activeUsers = int(G.nodes[nodeList[i]]['activeUsers'])\n",
    "        size.append(activeUsers)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node size according to degree\n",
    "def nodeSizeDegree(nodeList):\n",
    "    return [G.degree(node, weight='weight') for node in nodeList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node size according to occurances in user-wikis.txt\n",
    "def nodeSizeOccurances(nodeList):\n",
    "    size = []\n",
    "    for i in range(len(nodeList)):\n",
    "        if str(nodeList[i]) == \"nan\":\n",
    "            continue\n",
    "        usersOccurances = int(G.nodes[nodeList[i]]['usersOccurances'])*15  \n",
    "        size.append(usersOccurances)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the sizes and create a graph according to the different hubs, the method *createNodeSizes* was generated as well as *drawAndSaveGraph*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node sized for all hubs according to a specific methode\n",
    "def createNodeSizes(sizeMethode, hubs, betweenness_centrality=None):\n",
    "    node_size = {}\n",
    "    for hub in hubs.keys():\n",
    "        if betweenness_centrality==None:\n",
    "            node_size[hub] = sizeMethode(hubs[hub])\n",
    "        else:\n",
    "            node_size[hub] = sizeMethode(hubs[hub], betweenness_centrality)\n",
    "    return node_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drawing and saving the graph with different colored hubs and nodesizes\n",
    "def drawAndSaveGraph(hight, width, G, positions, hubs, node_size, max_size_lable, grapgName):\n",
    "    # plot figure\n",
    "    plt.figure(figsize=(hight, width))  \n",
    "\n",
    "    labels = {}    \n",
    "    for k,v in hubs.items():\n",
    "        hub_sizes = node_size[k]\n",
    "        for i in range(len(v)):\n",
    "            if hub_sizes[i] > max_size_lable:\n",
    "                #set the node name as the key and the label as its value \n",
    "                labels[v[i]] = v[i]\n",
    "\n",
    "\n",
    "    node_label = [node for node in G.nodes()]\n",
    "    colors = ['red', 'brown', 'yellow', 'orange', 'lightgreen', 'green', 'lightblue', 'pink']\n",
    "    \n",
    "    j = 0\n",
    "    for hub in hubs.keys():\n",
    "        nx.draw_networkx_nodes(G, positions, nodelist=hubs[hub], node_size=node_size[hub],cmap=plt.get_cmap('jet'), node_color=colors[j], label=hub)\n",
    "        j+=1\n",
    "    \n",
    "    nx.draw_networkx_edges(G, positions, width=0.1, cmap=plt.get_cmap('jet'), edge_color=\"gray\")\n",
    "    nx.draw_networkx_labels(G, positions, labels=labels, font_size=19, font_color='k', font_weight='bold', alpha=2.0)\n",
    "\n",
    "    plt.legend(numpoints = 1)\n",
    "    plt.title(grapgName)\n",
    "    #texts = [plt.text(n, labels[n], ha='center', va='center') for n in G.nodes()]\n",
    "    #adjust_text(texts)#, only_move='y', arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.savefig('graphs/'+grapgName+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets see the graphs!**\n",
    "\n",
    "There are three graphs in the following code blocks:\n",
    " * Graph with node size according to degree\n",
    " * Graph with node size according to the number of times a user from our dataset follows the wikis\n",
    " * Graph with node size according to the number of active users given in the wikia dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = createNodeSizes(nodeSizeDegree, hubs) \n",
    "for k,v in node_size.items():\n",
    "    print(k, max(v), min(v), np.average(v))\n",
    "\n",
    "drawAndSaveGraph(18, 16, G, positions, hubs, node_size, 270, 'wikiNetworkDegree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is a we expect. The nodes close to the middel are larger, than the ones at the edge. The largest and thereby those with the most degrees are are the nodes: \n",
    "* Hypnosis Mic Wiki (Music)\n",
    "* Miss Rose Wiki (TV)\n",
    "* Cousins for Life Wiki (TV)\n",
    "All nodes have a degree (with weight) above 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = createNodeSizes(nodeSizeOccurances, hubs) \n",
    "for k,v in node_size.items():\n",
    "    print(k, max(v), min(v), np.average(v))\n",
    "\n",
    "drawAndSaveGraph(18, 16, G, positions, hubs, node_size, 600, 'wikiNetworkUserOccurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is a we expect. The nodes close to the middel are larger, than the ones at the edge. The largest and thereby those with the most degrees are are the nodes: \n",
    "* Hypnosis Mic Wiki (Music)\n",
    "* Miss Rose Wiki (TV)\n",
    "* Cousins for Life Wiki (TV)\n",
    "All nodes have a degree (with weight) above 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = createNodeSizes(nodeSizeFrequency, hubs) \n",
    "for k,v in node_size.items():\n",
    "    print(k, max(v), min(v), np.average(v))\n",
    "\n",
    "drawAndSaveGraph(18, 16, G, positions, hubs, node_size, 760, 'wikiNetworkUserFrequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tools, theory and analysis\n",
    "\n",
    "Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "How did you use the tools to understand your dataset?\n",
    "\n",
    "\n",
    "I envision Part 3: **Tools, theory and analysis** as the central part of the assignment, where you basically go through the steps in the analysis. So the structure of this part would be something like\n",
    "\n",
    "* Explain the overall idea\n",
    "* Analysis step 1\n",
    "    * explain what you're interested in\n",
    "    * explain the tool\n",
    "    * apply the tool\n",
    "    * discuss the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Degree Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createHistigram(ylable, xlable, title, fileName, hubsDegree, bins):\n",
    "    # Create Bin Histogram \n",
    "    plt.figure(figsize=(12, 3))  \n",
    "    plt.style.use('seaborn-deep')\n",
    "    \n",
    "    maxDegree = 0\n",
    "    for v in hubsDegree.values():\n",
    "        if max(v)>maxDegree:\n",
    "            maxDegree = max(v)\n",
    "    bins = np.linspace(0, maxDegree, bins)\n",
    "\n",
    "    plt.hist(list(hubsDegree.values()), \n",
    "         bins, \n",
    "         label=list(hubsDegree.keys()), \n",
    "         histtype='bar',\n",
    "         edgecolor='black')\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylable)\n",
    "    plt.xlabel(xlable)\n",
    "    plt.savefig('graphs/'+fileName+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "degrees = {}\n",
    "degrees['all wikis'] = [val for (node, val) in G.degree()]\n",
    "\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution without weight\", \n",
    "                \"wikiDegreeSumHist\", \n",
    "                degrees,\n",
    "                100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "degrees = {}\n",
    "degrees['all wikis'] = [val for (node, val) in G.degree() if val>1]\n",
    "\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution without weight with larger degree than 1\", \n",
    "                \"wikiDegreelarger1SumHist\", \n",
    "                degrees,\n",
    "                100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "degrees = {}\n",
    "degrees['all wikis'] = [val for (node, val) in G.degree(weight='weight')]\n",
    "\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution with weight\", \n",
    "                \"wikiDegreeSumWeightHist\", \n",
    "                degrees,\n",
    "                100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "degrees = {}\n",
    "degrees['all wikis'] = [val for (node, val) in G.degree(weight='weight') if val>1]\n",
    "\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution with weight and larger degree than 1\", \n",
    "                \"wikiDegreeSumlarger1WeightHist\", \n",
    "                degrees,\n",
    "                100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubsDegree = createNodeSizes(nodeSizeDegree, hubs)\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution between hubs with weight\", \n",
    "                \"wikiDegreeWeightHist\", \n",
    "                hubsDegree, \n",
    "                10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubsDegreefilter = {}\n",
    "for k,v in hubsDegree.items():\n",
    "    hubsDegreefilter[k] = list(filter(lambda a: a > 2, v))\n",
    "\n",
    "createHistigram(\"Wikias\", \n",
    "                \"Number of common users\", \n",
    "                \"Degree Distribution between hubs wit weight and larger degree than 2\", \n",
    "                \"wikiDegreelarger2WeightHist\", \n",
    "                hubsDegreefilter, \n",
    "                10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCumulativeHistigram(ylable, xlable, title, fileName, hubsDegree):\n",
    "    \n",
    "    # Create Cumulative Step Histogram \n",
    "    plt.figure(figsize=(12, 3))  \n",
    "    plt.style.use('seaborn-deep')\n",
    "    \n",
    "    maxDegree = 0\n",
    "    for v in hubsDegree.values():\n",
    "        if max(v)>maxDegree:\n",
    "            maxDegree = max(v)\n",
    "    bins = np.linspace(0, maxDegree, 30)\n",
    "\n",
    "    plt.hist(list(hubsDegree.values()), \n",
    "         bins, \n",
    "         label=list(hubsDegree.keys()), \n",
    "         histtype='step', \n",
    "         cumulative=True)\n",
    "    #patch.linewidth\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylable)\n",
    "    plt.xlabel(xlable)\n",
    "    plt.savefig('graphs/'+fileName+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createCumulativeHistigram(\"Wikias\", \n",
    "                          \"Number of common users\", \n",
    "                          \"Degree Distribution between hubs without weight\", \n",
    "                          \"wikiDegreeCumulativeHist\", \n",
    "                          hubsDegree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Log-Log Degree Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degrees = [val for (node, val) in G.degree(weight='weight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "#Find and print min/max\n",
    "print(\"Max degree is %s\\nMin degree is %s\" % (max(degrees), min(degrees)))\n",
    "\n",
    "# Find bins and hist\n",
    "hist, binList = np.histogram(degrees, max(degrees))\n",
    "bins = (binList[1:]+binList[:-1])/2\n",
    "\n",
    "#Generate linear plot\n",
    "axs[0].plot(bins,hist, 'o', mfc='none')\n",
    "axs[0].set_ylabel('Number of Wikis')\n",
    "axs[0].set_xlabel('Number of degrees')\n",
    "axs[0].set_title(\"Linear\")\n",
    "\n",
    "\n",
    "#Generate log-log plot\n",
    "axs[1].loglog(bins,hist, 'o', mfc='none')\n",
    "axs[1].set_ylabel('Number of Wikis')\n",
    "axs[1].set_xlabel('Number of degrees')\n",
    "axs[1].set_title(\"Log transformed\")\n",
    "\n",
    "fig.suptitle(\"Degree Distributions\")\n",
    "plt.savefig('graphs/LogDegreeDistribution.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Compare to random Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degrees = [val for (node, val) in G.degree()]\n",
    "\n",
    "n = len(G.nodes())\n",
    "k = int(round(np.average(degrees)))\n",
    "m = 3\n",
    "seed = None\n",
    "\n",
    "# Create Randam graph\n",
    "G_random = nx.watts_strogatz_graph(n,k , 1)\n",
    "degrees_random = [val for (node, val) in G_random.degree(weight='weight')]\n",
    "\n",
    "# Create BA Graph\n",
    "G_ba= nx.barabasi_albert_graph(n, m, seed=seed)\n",
    "degrees_ba = [val for (node, val) in G_ba.degree(weight='weight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "\n",
    "#Find and print min/max\n",
    "print(\"Our: Number of nodes %s, Number of edges %s, Max degree is %s, Min degree is %s\" % \n",
    "      (len(G.nodes()), len(G.edges()), max(degrees), min(degrees)))\n",
    "print(\"BA:  Number of nodes %s, Number of edges %s, Max degree is %s, Min degree is %s\" % \n",
    "      (len(G_ba.nodes()), len(G_ba.edges()), max(degrees_ba), min(degrees_ba)))\n",
    "print(\"Ran: Number of nodes %s, Number of edges %s, Max degree is %s, Min degree is %s\" % \n",
    "      (len(G_random.nodes()), len(G_random.edges()), max(degrees_random), min(degrees_random)))\n",
    "\n",
    "#### Our graph ###\n",
    "hist, binList = np.histogram(degrees, max(degrees))\n",
    "bins = (binList[1:]+binList[:-1])/2\n",
    "\n",
    "dot1, = axs[0].plot(bins,hist, 'o', mfc='none')\n",
    "dotlog1, = axs[1].loglog(bins,hist, 'o', mfc='none')\n",
    "\n",
    "#### Random graph ###\n",
    "hist, binList = np.histogram(degrees_ba, max(degrees_ba))\n",
    "bins = (binList[1:]+binList[:-1])/2\n",
    "\n",
    "dot2, = axs[0].plot(bins,hist, 'o', mfc='none')\n",
    "dotlog2, = axs[1].loglog(bins,hist, 'o', mfc='none')\n",
    "\n",
    "\n",
    "#### BA graph ###\n",
    "hist, binList = np.histogram(degrees_random, max(degrees_random))\n",
    "bins = (binList[1:]+binList[:-1])/2\n",
    "\n",
    "dot3, = axs[0].plot(bins,hist, 'o', mfc='none')\n",
    "dotlog3, = axs[1].loglog(bins,hist, 'o', mfc='none')\n",
    "\n",
    "\n",
    "# finishing touches\n",
    "axs[0].legend([dot1, dot2, dot3], [\"Fandom Wikis\", \"BA\",\"Random\"])\n",
    "axs[0].set_ylabel('Number of Wikis')\n",
    "axs[0].set_xlabel('Number of degrees')\n",
    "axs[0].set_title(\"Linear\")\n",
    "\n",
    "axs[1].legend([dotlog1, dotlog2, dotlog3], [\"Fandom Wikis\", \"BA\",\"Random\"])\n",
    "axs[1].set_ylabel('Number of Wikis')\n",
    "axs[1].set_xlabel('Number of degrees')\n",
    "axs[1].set_title(\"Log transformed\")\n",
    "\n",
    "fig.suptitle(\"Comparison of Degree Distributions\")\n",
    "plt.savefig('graphs/DegreeDistributionComparison.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 Shortest Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_graphs = []\n",
    "for i in range(1,7):\n",
    "    connected_graphs.append([c for c in nx.connected_component_subgraphs(G) if len(c)==i])\n",
    "connected_graphs.append([c for c in nx.connected_component_subgraphs(G) if len(c)>6])\n",
    "connected_graphs\n",
    "\n",
    "print(\"The overall network contains:\")\n",
    "for graphList in connected_graphs:\n",
    "    print(\" - %s nodes per graph, total amount of graphs %s, with average shortest path len %s\" % \n",
    "        (len(graphList[0].nodes()), #np.average([len(g.nodes()) for g in graphList]),\n",
    "         len(graphList),\n",
    "         np.average([nx.average_shortest_path_length(x) for x in graphList])))\n",
    "\n",
    "print(\"The average shortest path length for graph BA (nodes: %s) is %s\" % \n",
    "      (len(G_ba.nodes()),\n",
    "       nx.average_shortest_path_length(G_ba)))\n",
    "print(\"The average shortest path length for graph Random (nodes: %s) is %s\" %\n",
    "      (len(G_random.nodes()),\n",
    "       nx.average_shortest_path_length(G_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.time()\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "print(hub, time.time()-current_time)\n",
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node size according to Betweeness Centrality\n",
    "def nodeSizeBetweennessCentrality(nodeList, betweenness_centrality):\n",
    "    return [betweenness_centrality[node]*100000 for node in nodeList]\n",
    "\n",
    "node_size = createNodeSizes(nodeSizeBetweennessCentrality, hubs, betweenness_centrality) \n",
    "\n",
    "for k,v in node_size.items():\n",
    "    print(k, max(v), min(v), np.average(v))\n",
    "\n",
    "drawAndSaveGraph(18, 16, G, positions, hubs, node_size, 500, 'wikiNetworkBetweenessCentrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node size according to Eigenvector Centrality\n",
    "def nodeSizeEigenvectorCentrality(nodeList):\n",
    "    eigenvector_centrality = nx.eigenvector_centrality_numpy(G, weight='weight')\n",
    "    return [eigenvector_centrality[node]*5000 for node in nodeList]\n",
    "\n",
    "\n",
    "node_size = createNodeSizes(nodeSizeEigenvectorCentrality, hubs) \n",
    "\n",
    "for k,v in node_size.items():\n",
    "    print(k, max(v), min(v), np.average(v))\n",
    "\n",
    "drawAndSaveGraph(18, 16, G, positions, hubs, node_size, 516, 'wikiNetworkEigenvectorCentrality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5 Communities + matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_connected = connected_graphs[6][0]\n",
    "\n",
    "#Community partitions\n",
    "partition = cm.best_partition(G_connected)\n",
    "\n",
    "#Modularity\n",
    "mod = cm.modularity(partition, G_connected)\n",
    "print('Modularity using the Louvain algorithm:', mod)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "colors = np.asarray(['#104E8B','#EE2C2C','#db3aAA','#EEC900','#32BBAA',\n",
    "                     '#4020AA','#084c61','#407058','#6495ED','#C1FFC1',\n",
    "                     '#8b104e','#4e8b10','#bd861e','#901c5c','#b68bb2',\n",
    "                     '#57235a','#8517b6','#8f7cce','#5628ee','#708dae',\n",
    "                     '#70a6ae','#26b0a5','#0f9440','#948e0f','#dd8e3f'])\n",
    "i = 0\n",
    "for com in set(partition.values()):\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    \n",
    "    nodes = [nodes for nodes in list_nodes]\n",
    "\n",
    "    if len(list_nodes)>5: \n",
    "        color = colors[i]\n",
    "        i += 1\n",
    "    else: \n",
    "        color = \"#97FFFF\"\n",
    "\n",
    "    nx.draw_networkx_nodes(G_connected, positions, nodes, node_size = 75, node_color = color, label=com)\n",
    "\n",
    "plt.legend(numpoints = 1)\n",
    "nx.draw_networkx_edges(G_connected, positions, width = 0.2, alpha=0.5, edge_color='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('graphs/communities.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Define a community size function to sort the community by size computing the confusion matrix.\n",
    "def community_Size(com):\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    return len(list_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.zeros((len(hubs), len(set(partition.values()))),dtype = int) # Define the matrix\n",
    "sorted_partition = sorted(set(partition.values()), key = community_Size, reverse =True)\n",
    "\n",
    "i=0 # Set a count\n",
    "for com in sorted_partition:  # Go through the partition\n",
    "    j = 0\n",
    "    hub_node = {}\n",
    "    for key in hubs.keys():\n",
    "        hub_node[key] = [node for node in partition.keys() \n",
    "                           if partition[node] == com and key in G.node[node][\"hub\"]]\n",
    "\n",
    "        D[j,i]=len(hub_node[key]) # line 0 corresponds to the republicans\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "im= plt.imshow(D, cmap=pltcm.Blues)\n",
    "plt.title(\"Full confusion matrix compared with hubs\")\n",
    "plt.yticks([i for i in range(len(hubs.keys()))], hubs.keys(), rotation='horizontal')\n",
    "plt.xticks([i for i in range(len(sorted_partition))], sorted_partition, rotation='horizontal')\n",
    "plt.ylabel(\"Hubs\")\n",
    "plt.xlabel(\"Communities\")\n",
    "\n",
    "fig.colorbar(im, orientation = \"horizontal\", pad=0.2)\n",
    "plt.savefig('graphs/Matrix_hub.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set([G.node[n]['topic'] for n in G.nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_hub = np.zeros((len(hubs),1)) # Define the matrix\n",
    "\n",
    "i=0 # Set a count\n",
    "hub_node={}\n",
    "for h in hubs.keys():\n",
    "    hub_node[h] = [node for node in G_connected.nodes() \n",
    "                           if h in G_connected.node[node][\"hub\"]]\n",
    "\n",
    "    D_hub[i,0]=len(hub_node[h]) # line 0 corresponds to the republicans\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "im= plt.imshow(D_hub, cmap=pltcm.Blues)\n",
    "plt.title(\"Distribution of wikis\\nin hubs\")\n",
    "plt.yticks([i for i in range(len(hubs.keys()))], hubs.keys(), rotation='horizontal')\n",
    "plt.xticks([0], ['Distribution'], rotation='horizontal')\n",
    "plt.ylabel(\"Hubs\")\n",
    "\n",
    "fig.colorbar(im, orientation = \"vertical\", pad=0.05)\n",
    "plt.savefig('graphs/Matrix_hub_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = [t for t in set(activeDataSet['topic']) if str(t) != 'nan']\n",
    "topics.append('')\n",
    "\n",
    "D = np.zeros((len(topics), len(set(partition.values()))),dtype = int) # Define the matrix\n",
    "\n",
    "i=0 # Set a count\n",
    "for com in sorted_partition:  # Go through the partition\n",
    "    \n",
    "    j = 0\n",
    "    topic_node = {}\n",
    "    for t in topics:\n",
    "        if t != '':\n",
    "            topic_node[t] = [node for node in partition.keys() \n",
    "                               if partition[node] == com and t in G.node[node][\"topic\"]]\n",
    "        else:\n",
    "            topic_node[t] = [node for node in partition.keys() \n",
    "                               if partition[node] == com and t == G.node[node][\"topic\"]]\n",
    "\n",
    "        D[j,i]=len(topic_node[t]) # line 0 corresponds to the republicans\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "im= plt.imshow(D, cmap=pltcm.Blues)\n",
    "plt.title(\"Full confusion matrix compared with topics\")\n",
    "plt.yticks([i for i in range(len(topics))], topics, rotation='horizontal')\n",
    "plt.xticks([i for i in range(len(sorted_partition))], sorted_partition, rotation='horizontal')\n",
    "plt.ylabel(\"Topics\")\n",
    "plt.xlabel(\"Communities\")\n",
    "\n",
    "fig.colorbar(im, orientation = \"vertical\", pad=0.05)\n",
    "plt.savefig('graphs/Matrix_topic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_topic = np.zeros((len(topics),1)) # Define the matrix\n",
    "\n",
    "i=0 # Set a count\n",
    "topic_node = {}\n",
    "for t in topics:\n",
    "    if t != '':\n",
    "        topic_node[t] = [node for node in G_connected.nodes() \n",
    "                            if t in G_connected.node[node][\"topic\"]]\n",
    "    else:\n",
    "        topic_node[t] = [node for node in G_connected.nodes() \n",
    "                            if t == G_connected.node[node][\"topic\"]]\n",
    "\n",
    "    D_topic[i,0]=len(topic_node[t]) # line 0 corresponds to the republicans\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "im= plt.imshow(D_topic, cmap=pltcm.Blues)\n",
    "plt.title(\"Distribution of wikis\\nin topics\")\n",
    "plt.yticks([i for i in range(len(topics))], topics, rotation='horizontal')\n",
    "plt.xticks([0], ['Distribution'], rotation='horizontal')\n",
    "plt.ylabel(\"Topics\")\n",
    "\n",
    "fig.colorbar(im, orientation = \"vertical\", pad=0.05)\n",
    "plt.savefig('graphs/Matrix_topic_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.6 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " data/articles/text_from_wikis/Books-articles-part-1.json\n",
      "250, 183, \n",
      " data/articles/text_from_wikis/Comics-articles-part-1.json\n",
      "250, 104, \n",
      " data/articles/text_from_wikis/Games-articles-part-1.json\n",
      "250, 87, 250, 250, 250, 250, 250, 250, 250, 198, \n",
      " data/articles/text_from_wikis/Lifestyle-articles-part-1.json\n",
      "250, 250, 174, \n",
      " data/articles/text_from_wikis/Movies-articles-part-1.json\n",
      "250, 158, \n",
      " data/articles/text_from_wikis/Music-articles-part-1.json\n",
      "192, \n",
      " data/articles/text_from_wikis/Other-articles-part-1.json\n",
      "10, \n",
      " data/articles/text_from_wikis/TV-articles-part-1.json\n",
      "250, 250, 250, 241, 88, "
     ]
    }
   ],
   "source": [
    "# Divide data into hubs\n",
    "hub_articles_wiki = {}\n",
    "hub_keys = ['TV', 'Music', 'Lifestyle', 'Books', 'Movies', 'Other', 'Games', 'Comics']\n",
    "\n",
    "## Get Article Data ##\n",
    "for dataPath in glob.glob(\"data/articles/text_from_wikis/*.json\"):\n",
    "    hub = 'Not found'\n",
    "    \n",
    "    # find files hub\n",
    "    for h in hub_keys:\n",
    "        if h in dataPath:\n",
    "            hub = h\n",
    "    \n",
    "    # initialise directory\n",
    "    if hub not in hub_articles_wiki:\n",
    "        print('\\n', dataPath)\n",
    "        hub_articles_wiki[hub] = {}\n",
    "    \n",
    "    # load file\n",
    "    with open(dataPath) as f:\n",
    "         text = json.loads(f.read())\n",
    "    \n",
    "    # merge dirctory wiki data into the hub directory.\n",
    "    hub_articles_wiki[hub].update(text)\n",
    "    \n",
    "    # control correct/all data\n",
    "    print(len(text), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books ['percy'] ['scary'] ['wwii'] 432\n",
      "Comics ['another'] ['comic'] ['canadian'] 347\n",
      "Games ['renesanční'] ['visitors'] ['cardfight'] 2204\n",
      "Lifestyle ['logopedia'] ['otherkin'] ['brother'] 669\n",
      "Movies ['电影维基_wikia'] ['right'] ['comics'] 398\n",
      "Music ['music'] ['music'] [] 192\n",
      "Other ['soul'] ['community'] ['centrum'] 10\n",
      "TV ['radio'] ['中文電視大典'] ['greek'] 1048\n"
     ]
    }
   ],
   "source": [
    "### Sort data ###\n",
    "\n",
    "# List of stop words, punctuation marks and unicode hex characters\n",
    "encodings = ['\\xef', '\\xe2', '\\xcc', '\\xe3', '\\xf0', 'i\\xe2', 't\\xe2', 'a\\xe2', 'it\\xe2', 's\\xe2', 'we\\xe2', 'and\\xe2', 'the\\xe2']\n",
    "uninteresting_common_words = ['one', 'also', 'first', 'time', 'however', 'wiki', 'wikia', 'back']\n",
    "stop_punct = set(stopwords.words('english')\n",
    "                 + list(string.punctuation) \n",
    "                 + get_stop_words('en')\n",
    "                 + encodings\n",
    "                 + uninteresting_common_words)\n",
    "\n",
    "# Tokenize the text in both files and filter the stopwords, punctuation marks and unicode hex characters\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Filter data\n",
    "hub_articles_wiki_filtered = {}\n",
    "for k,v in hub_articles_wiki.items():\n",
    "    # initialize filter\n",
    "    if k not in hub_articles_wiki_filtered:\n",
    "            hub_articles_wiki_filtered[k] = {}\n",
    "    \n",
    "    # Go though all wikis in hub\n",
    "    for wiki,text in v.items():\n",
    "        # remove links and make lower case\n",
    "        regx_list = ['(http|https):\\/\\/\\w*.\\w*\\/\\w*', '\\d']\n",
    "        for regx in regx_list:\n",
    "            text = re.sub(regx, '', text).lower()\n",
    "        # tokenize data\n",
    "        filter_hub = tokenizer.tokenize(text)\n",
    "        # remove words with length less than 3 and stop words, etc.\n",
    "        hub_articles_wiki_filtered[k][wiki] = [word for word in filter_hub if word not in stop_punct and len(word)>3]\n",
    "    \n",
    "    #First 1 element for the first 3 wikis in the filtered tokenized list for every hub\n",
    "    print(k, end=' ')\n",
    "    [print(hub_articles_wiki_filtered[k][wiki][:1], end=' ') for wiki in list(hub_articles_wiki_filtered[k].keys())[:3]]\n",
    "    print(len(hub_articles_wiki_filtered[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books 1360533\n",
      "Comics 1717111\n",
      "Games 4106305\n",
      "Lifestyle 2013806\n",
      "Movies 1897893\n",
      "Music 390913\n",
      "Other 17289\n",
      "TV 4985331\n"
     ]
    }
   ],
   "source": [
    "### Merge wikis together to hubs ###\n",
    "hub_articles = {}\n",
    "\n",
    "for k,v in hub_articles_wiki_filtered.items():\n",
    "    if k not in hub_articles:\n",
    "        hub_articles[k] = []\n",
    "    for wiki_text in v.values():\n",
    "        hub_articles[k] = hub_articles[k]+wiki_text\n",
    "    print(k, len(hub_articles[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to calculate TF / IDF / TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTF(wordList):\n",
    "    FD = nltk.FreqDist(wordList)\n",
    "    TF = []\n",
    "    n =  float(len(wordList))\n",
    "    for i in FD.most_common(len(FD)):\n",
    "        TF.append((i[0], float(i[1])/n))\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIDF(documentList):\n",
    "    all_words = []\n",
    "    for dokument in documentList.values():\n",
    "        all_words = all_words + dokument\n",
    "        all_words = list(set(all_words))\n",
    "    \n",
    "    print('All words in every wiki: ',len(all_words))\n",
    "    \n",
    "    IDF = []\n",
    "    n = math.log(float(len(documentList)))\n",
    "    for word in all_words:\n",
    "        contains = sum(1 for dokument in documentList.values() if word in set(dokument))\n",
    "        IDF.append((word, (n / float(1 + contains))))\n",
    "        if len(IDF) % 500 == 0:\n",
    "            print(len(IDF), end=', ')\n",
    "\n",
    "    IDF.sort(key=lambda tup: tup[1],reverse=True)\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTF_IDF(TF_List, IDF_List):\n",
    "    TF_IDF = []\n",
    "    for TF in TF_List:\n",
    "        IDF = [idf[1] for idf in IDF_List if idf[0] == TF[0]]\n",
    "        TF_IDF.append((TF[0], TF[1]*IDF))\n",
    "    TF_IDF.sort(key=lambda tup: tup[1],reverse=True)\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF for hub:\n",
    "hub = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books 1360533 66261\n",
      "  [('harry', 0.00342659825230259), ('like', 0.0027356925557851224), ('later', 0.0024732953923205098)]\n",
      "Comics 1717111 53160\n",
      "  [('light', 0.0035309307319095853), ('death', 0.003329429489415652), ('like', 0.0030906563407956736)]\n",
      "Games 4106305 115358\n",
      "  [('game', 0.005990787338008258), ('player', 0.003028026413040434), ('like', 0.0027336011328919795)]\n",
      "Lifestyle 2013806 186201\n",
      "  [('states', 0.002528048878591086), ('united', 0.002286714807682567), ('many', 0.0020448841646116857)]\n",
      "Movies 1897893 58202\n",
      "  [('jedi', 0.003849532086371571), ('film', 0.0026871904791260625), ('skywalker', 0.0026866635790321163)]\n",
      "Music 390913 31354\n",
      "  [('album', 0.011053610394128617), ('band', 0.00808364009383162), ('released', 0.006001335335483854)]\n",
      "Other 17289 6004\n",
      "  [('chocolate', 0.019376482156284344), ('milk', 0.008907397767366534), ('pages', 0.005032101336109665)]\n",
      "TV 4985331 97058\n",
      "  [('later', 0.0035173191108072866), ('season', 0.0030431279287172707), ('tells', 0.0028740318345963386)]\n"
     ]
    }
   ],
   "source": [
    "# hub = docutment\n",
    "TF_hub = {}\n",
    "\n",
    "#Calculates the TF for each word\n",
    "for k,v in hub_articles.items():\n",
    "    TF_hub[k] = calculateTF(v)\n",
    "    \n",
    "    #The 3 words with the highest term frequency for both Democratic and Republican tweets:\n",
    "    print(k, len(hub_articles[k]), len(TF_hub[k]))\n",
    "    print(' ', TF_hub[k][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words in every wiki:  358100\n",
      "500, "
     ]
    }
   ],
   "source": [
    "# hub = docutment\n",
    "\n",
    "#Calculates the IDF for each word\n",
    "IDF_hub = calculateIDF(hub_articles)\n",
    "\n",
    "    #The 3 words with the highest inverse document frequency for both Democratic and Republican tweets:\n",
    "print(k, end=' ')\n",
    "[print(IDF_hub[word]) for word in list(IDF_hub.values())[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hub = docutment\n",
    "TF_IDF = {}\n",
    "\n",
    "#Calculates the TF-IDF for each word\n",
    "for k,v in TF_hub.items():\n",
    "    TF_IDF[k] = calculateTF_IDF(v, IDF_hub)\n",
    "    \n",
    "    print(k, len(TF_IDF[k]))\n",
    "    [print(' ', TF_IDF[k]) for i in range(2)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF for wikis:\n",
    "wiki = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# wiki = docutment\n",
    "TF_hub = {}\n",
    "\n",
    "#Calculates the TF for each word\n",
    "for k,v in hub_articles_wiki_filtered.items():\n",
    "    if k not in TF_hub:\n",
    "        TF_hub[k] = {}\n",
    "    \n",
    "    for wiki, text in v.items():\n",
    "        TF_hub[k][wiki] = calculateTF(text)\n",
    "    \n",
    "    #The 3 words with the highest term frequency for both Democratic and Republican tweets:\n",
    "    print(k, len(TF_hub[k]))\n",
    "    [print(' ', TF_hub[k][i][:2], len(TF_hub[k][i])) for i in list(TF_hub[k].keys())[:3]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# wiki = docutment\n",
    "IDF_hub = {}\n",
    "for k,v in hub_articles_wiki_filtered.items():\n",
    "    #Calculates the IDF for each word\n",
    "    IDF_hub[k] = calculateIDF(v)\n",
    "\n",
    "    #The 3 words with the highest inverse document frequency for both Democratic and Republican tweets:\n",
    "    print(k, IDF_hub[k][:3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.7 Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"word\", \"happiness_rank\",\"happiness_average\", \"happiness_standard_deviation\", \n",
    "       \"twitter_rank\", \"google_rank\", \"nyt_rank\", \"lyrics_rank\"]\n",
    "\n",
    "dsS1 = pd.read_csv('data/journal.pone.0026752.s001.TXT', header=None, delimiter=\"\\t\", names=names, skiprows=[0,1,2,3])\n",
    "dsS1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens is a list of tokens\n",
    "# ds is the data set S1 as a dataframe\n",
    "def calculates_sentiment(tokens, ds):\n",
    "    print(len(tokens))\n",
    "    if len(tokens)==0:\n",
    "        return 0\n",
    "    \n",
    "    keys = [key.lower() for key in tokens if key.lower() in ds['word'].values]\n",
    "    \n",
    "    print(len(keys))\n",
    "    normalization = len(keys)\n",
    "    if normalization==0:\n",
    "        return 0\n",
    "    \n",
    "    sums = sum(ds.loc[ds['word'] == key]['happiness_average'].values[0] for key in keys)\n",
    "    return sums/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_hub = {}\n",
    "for k,v in hub_articles_filtered.items():\n",
    "    i = 0\n",
    "    sentiment_hub[k] = []\n",
    "    print(k)\n",
    "    sentiment_hub[k] = (calculates_sentiment(v, dsS1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Histogram \n",
    "plt.figure(figsize=(12, 5))  \n",
    "plt.style.use('seaborn-deep')\n",
    "bins = np.linspace(0, np.max([np.max(sentiment_repu), np.max(sentiment_demo)]), 50)\n",
    "\n",
    "plt.hist([sentiment_repu, sentiment_demo], \n",
    "         bins, \n",
    "         label=['Republican tweet sentiment', 'Democratic tweet sentiment'], \n",
    "         histtype='bar', \n",
    "         cumulative=False)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Cumulative distribution of outgoing strength for the republican and democratic nodes\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.xlabel(\"Sentiment (low is negative, high is positive)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_d = np.average(sentiment_demo)\n",
    "o_d = np.std(sentiment_demo)\n",
    "\n",
    "m_r = np.average(sentiment_repu)\n",
    "o_r = np.std(sentiment_repu)\n",
    "\n",
    "print(\"Demotratics: average: %s, std: %s\" % (m_d, o_d))\n",
    "print(\"Republicans: average: %s, std: %s\" % (m_r, o_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.8 WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wordcloud = WordCloud(background_color='white').generate(textDem)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.title('Word-cloud for Democratic tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show() \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Discussion\n",
    "\n",
    "* What went well?,\n",
    "* What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
