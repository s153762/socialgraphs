{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import collections\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from fa2 import ForceAtlas2\n",
    "import numpy as np\n",
    "import datetime\n",
    "from lxml import html\n",
    "import glob\n",
    "import ast # string to dirc\n",
    "import math # check if float is NaN\n",
    "# conda install -c phlya adjusttext \n",
    "#from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Save inital data in top-wikis-date #####\n",
    "n = 250\n",
    "\n",
    "# Initial query - get top wikis by pageviews using API\n",
    "query = \"http://www.wikia.com/api/v1/Wikis/List?expand=1&limit=\"+str(n)+\"&batch=1\"\n",
    "response = urlopen(query)\n",
    "wikisource = response.read()\n",
    "data = json.loads(wikisource)\n",
    "\n",
    "# Save data at data/wikis/top-wikis-date.json\n",
    "date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "with open(\"data/wikis/top-wikis-\"+date+\".json\", 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Read initial wiki data  ######\n",
    "date= \n",
    "df = pd.DataFrame()\n",
    "with open(\"data/wikis/top-wikis-\"+date+\".json\") as f:\n",
    "    json_file = json.loads(f.read())\n",
    "df = pd.DataFrame(json_file['items']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search for more wikis (primarily based on the different hub names)\n",
    "def saveSearchWikis(search, n):\n",
    "    querySearch = \"http://www.wikia.com/api/v1/Wikis/ByString?expand=1&string=\"+str(search)+\"&limit=\"+str(n)+\"&batch=1&includeDomain=true\"\n",
    "    response = urlopen(querySearch)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    if data['total'] > 0:\n",
    "        date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "        searchWikiPath = \"data/wikis/search-\"+search+\"-wikis-\"+date+\".json\"\n",
    "        with open(searchWikiPath, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "    else:\n",
    "        print(\"Data not found in %s: %s\" % (search, data['items']))\n",
    "    return pd.DataFrame(data['items']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Use hubs in dataset to searsh for new wikis and save as search-hub-wikis ##### \n",
    "fullDataSet = pd.DataFrame()\n",
    "for hub in set(df[\"hub\"]):\n",
    "    df = saveSearchWikis(hub, n)\n",
    "    fullDataSet = fullDataSet.append(df)\n",
    "    print(\"%s wikis found in %s\" % (str(len(df)), hub))\n",
    "fullDataSet = fullDataSet.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search online to find the username based on the userID\n",
    "def saveUsernames(keyword):\n",
    "    keyword = str(keyword)\n",
    "    querySearch = \"http://community.wikia.com/wiki/Special:Search?search=\"+keyword+\"&fulltext=Search&ns2=1\"\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    usernames = re.findall(\"wiki\\/User:([(\\w)]+)?\",str(wikisource))\n",
    "    \n",
    "    # Read exsisting data to avoid duplicates\n",
    "    filename = \"data/users/userNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    \n",
    "    # Print new usernames\n",
    "    for user in set(usernames):\n",
    "        userData = user+\"\\n\"\n",
    "        if userData not in contents: # not a duplicate\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(userData)\n",
    "            fileWrite.close()\n",
    "\n",
    "    return list(set(usernames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def userIDtoUserName(userID):\n",
    "    if userID == None:\n",
    "        print(\"user ID is %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    if int(userID) < 1:\n",
    "        print(\"user ID less than 1: %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    userID = str(userID)\n",
    "    querySearch = \"http://www.wikia.com/api/v1/User/Details?ids=\"+userID\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    data = json.loads(wikisource)\n",
    "    \n",
    "    usernames = []\n",
    "    # Print new usernames\n",
    "    for name in data['items']:\n",
    "        usernames.append(name['title'])\n",
    "    return usernames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search online to find the username based on the userID\n",
    "def saveUsernamesByUserID(userID):\n",
    "    \n",
    "    if userID == None:\n",
    "        print(\"user ID is %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    if int(userID) < 1:\n",
    "        print(\"user ID less than 1: %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    userID = str(userID)\n",
    "    querySearch = \"http://www.wikia.com/api/v1/User/Details?ids=\"+userID\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    data = json.loads(wikisource)\n",
    "    \n",
    "    # Read exsisting data to avoid duplicates\n",
    "    filename = \"data/users/userNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    \n",
    "    # Print new usernames\n",
    "    for name in data['items']:\n",
    "        userData = name['title']+\" \"+name['user_id']+\"\\n\"\n",
    "        if userData not in contents: # not a duplicate\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(userData)\n",
    "            fileWrite.close()\n",
    "        elif name['title'] in contents:\n",
    "            content.replace(name['title'],userData)\n",
    "            fileWrite = open(filename,\"w\")\n",
    "            fileWrite.write(content)\n",
    "            fileWrite.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search online to find the wiki names which the user use, \n",
    "# This wiki saves the users wikis and all the wiki names\n",
    "def saveNewWikisAndUsersThroughUser(usernames):\n",
    "    wikis = []\n",
    "    for username in usernames:\n",
    "        \n",
    "        # encode username correct\n",
    "        username = str(username.encode('utf-8'))\n",
    "        username = username.replace(\"'\", \"\")\n",
    "        username = username.replace(\"b\", \"\")\n",
    "        \n",
    "        # find users activity\n",
    "        querySearch = \"http://community.wikia.com/index.php?limit=1000&tagfilter=&title=Special%3AContributions&target=\"+username+\"&namespace=&tagfilter=&year=&month=-1.html\"\n",
    "        response = urlopen(querySearch)\n",
    "        wikisource = response.read()\n",
    "        \n",
    "        \n",
    "        # search html for wikis\n",
    "        usersWikis = re.findall(\"wiki/Adoption:(\\w+)?\",str(wikisource))\n",
    "        usersWikis = [wiki for wiki in usersWikis if (wiki != \"Requests\" and wiki != '')]\n",
    "        usersWikis = list(set(usersWikis))\n",
    "        \n",
    "        # Save users and the individual user's wikis \n",
    "        saveUsersWikis(username, usersWikis)\n",
    "        \n",
    "        [wikis.append(wiki) for wiki in usersWikis]\n",
    "        wikis = list(set(wikis))\n",
    "        \n",
    "    # Save all found wiki names and read exsisting data to avoid duplicates\n",
    "    filename = \"data/wikis/wikiNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    for wiki in set(wikis):\n",
    "        wikiData = wiki+\"\\n\"\n",
    "        if wikiData not in contents:\n",
    "            # Print new usernames\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(wikiData)\n",
    "            fileWrite.close()\n",
    "    \n",
    "    return wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collectAndSaveData(userID):\n",
    "    # Get and save all usernames\n",
    "    saveUsernamesByUserID(userID)\n",
    "    \n",
    "    # Get and save all wikis found connected to the usernames\n",
    "    wikiList = saveNewWikisAndUsersThroughUser(username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove inactive wikis and duplicates\n",
    "def findActiveWikis(dataSet):\n",
    "    activeDataSet = pd.DataFrame()\n",
    "    dataSet = dataSet.drop_duplicates(subset=\"id\")\n",
    "    # find stats in dataset\n",
    "    for s in dataSet['stats']:\n",
    "        # only add when at least 1 active\n",
    "        print(s['activeUsers'])\n",
    "        if int(s['activeUsers'])>1:\n",
    "            data = dataSet.loc[dataSet['stats']==s]\n",
    "\n",
    "            activeDataSet = activeDataSet.append(data)#    \n",
    "    # remove duplicates\n",
    "    #activeDataSet = activeDataSet.reset_index(drop=True)\n",
    "    return activeDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get active wikis\n",
    "activeDataSet = findActiveWikis(fullDataSet)\n",
    "print(\"full data set has %s data, while only %s is actively used.\" % (len(fullDataSet),len(activeDataSet)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Find more users and their connected Wikis\n",
    "# find topusers\n",
    "topUsers = [x for x in activeDataSet[\"topUsers\"]]\n",
    "topUsers = itertools.chain.from_iterable(topUsers)\n",
    "\n",
    "# find founders\n",
    "founders = [x for x in activeDataSet['founding_user_id']]\n",
    "\n",
    "# all the different userID\n",
    "users = list(set(topUsers + founders))\n",
    "\n",
    "# find the wiki names of the users\n",
    "userWiki = {}\n",
    "for user in users:\n",
    "    for k, v in fullDataSet.T.items():\n",
    "        if user == v[\"topUsers\"] or user == v['founding_user_id']:\n",
    "            if user not in userWiki:\n",
    "                userWiki[user] = []\n",
    "            userWiki[user].append(v['name'])\n",
    "\n",
    "userWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find additional users connections\n",
    "\n",
    "for i in range(0,len(users)):\n",
    "    collectAndSaveData(users[i])\n",
    "    if i % 500 == 0:\n",
    "        print(i, len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### add Wiki Names ######\n",
    "# 12.21\n",
    "# Read exsisting data to avoid duplicates\n",
    "wikiNames = []\n",
    "filename = \"data/wikis/wikiNames.txt\"\n",
    "f = open(filename, \"r\")\n",
    "for line in f.readlines():\n",
    "    wikiNames.append(line.replace(\"\\n\",\"\"))\n",
    "f.close() \n",
    "\n",
    "i = 0\n",
    "for wikiName in wikiNames:\n",
    "    i+=1\n",
    "    df = saveSearchWikis(wikiName, n)\n",
    "    fullDataSet = fullDataSet.append(df)\n",
    "    if i%100==0:\n",
    "        print(\"%i af %s\" % (i,len(wikiNames)))\n",
    "\n",
    "fullDataSet = fullDataSet.drop_duplicates(subset=\"name\")    \n",
    "fullDataSet = fullDataSet.reset_index(drop=True)    \n",
    "len(fullDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Read Wiki Names ######\n",
    "# 22:49 - \n",
    "wikiDataPath = [f for f in glob.glob(\"data/wikis/*.json\")]\n",
    "print(len(wikiDataPath))\n",
    "i = 0\n",
    "for dataPath in wikiDataPath:\n",
    "    i+=1\n",
    "    with open(dataPath) as f:\n",
    "        json_file = json.loads(f.read())\n",
    "    fullDataSet = fullDataSet.append(pd.DataFrame(json_file['items']))\n",
    "    if(i%200 ==0):\n",
    "        print(i)\n",
    "\n",
    "fullDataSet = fullDataSet.drop_duplicates(subset=\"id\")    \n",
    "fullDataSet = fullDataSet.reset_index(drop=True) \n",
    "\n",
    "print(len(fullDataSet))\n",
    "fullDataSet = findActiveWikis(fullDataSet)\n",
    "print(len(fullDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Collected data\n",
    "date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "fullDataSet.to_csv(\"data/sortedWikiData-\"+date+\".csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove inactive wikis and duplicates\n",
    "def findActiveWikis(dataSet):\n",
    "    activeDataSet = pd.DataFrame()\n",
    "    dataSet = dataSet.drop_duplicates(subset=\"id\")\n",
    "    # find stats in dataset\n",
    "    for s in dataSet['stats']:\n",
    "        # only add when at least 1 active\n",
    "        sDirc = ast.literal_eval(s)\n",
    "        if int(sDirc['activeUsers'])>1:\n",
    "            data = dataSet.loc[dataSet['stats']==s]\n",
    "            activeDataSet = activeDataSet.append(data)#    \n",
    "    # remove duplicates\n",
    "    #activeDataSet = activeDataSet.reset_index(drop=True)\n",
    "    return activeDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5849"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get Data from CSV ###\n",
    "\n",
    "fullDataSet = pd.read_csv(\"data/sortedWikiData.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Filter Data ###\n",
    "activeDataSet = findActiveWikis(fullDataSet)\n",
    "len(activeDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22472"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find more users and their connected Wikis\n",
    "# find topusers\n",
    "topUsers = []\n",
    "for users in activeDataSet[\"topUsers\"]:\n",
    "    userList = makeStringToList(users)\n",
    "    for user in userList: \n",
    "        topUsers.append(str(int(user)))\n",
    "\n",
    "# find founders\n",
    "founders = []\n",
    "for user in activeDataSet[\"founding_user_id\"]:\n",
    "    user = floatToString(user)\n",
    "    if int(user) > -1:\n",
    "        founders.append(user)\n",
    "\n",
    "# all the different userID\n",
    "users = list(set(topUsers + founders))\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, "
     ]
    },
    {
     "data": {
      "text/plain": [
       "22472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the wiki names of the users\n",
    "userWiki = {}\n",
    "i = 0\n",
    "for user in users:\n",
    "    i+= 1\n",
    "    for k, v in activeDataSet.T.items():\n",
    "        topUsers = makeStringToList(v[\"topUsers\"])\n",
    "        if user in topUsers or user == floatToString(v['founding_user_id']):\n",
    "            if user not in userWiki:\n",
    "                if i % 500 == 0:\n",
    "                    print(i, end=\", \")\n",
    "                userWiki[user] = []\n",
    "            userWiki[user].append(v['name'])\n",
    "            \n",
    "len(userWiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user ID less than 1: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for k, v in userWiki.items():\n",
    "    name = userIDtoUserName(k)\n",
    "    if len(name) > 0:\n",
    "        saveUsersWikis(name[0], v)\n",
    "    else:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving new users in file user.wikis\n",
    "def saveUsersWikis (username, userWikis):\n",
    "    userData = username+\": \"+str(userWikis)+\"\\n\"\n",
    "        \n",
    "    filename = \"data/users/user-wikis.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents =fileRead.read()\n",
    "        \n",
    "    # Ensure no duplicates\n",
    "    if userData not in contents: # not a duplicate\n",
    "        fileWrite = open(filename,\"a\")\n",
    "        fileWrite.write(userData)\n",
    "        fileWrite.close()\n",
    "        fileRead.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeStringToList(string):\n",
    "    if string == \"[]\":\n",
    "        return []\n",
    "    l = string.replace(\"[\",\"\")\n",
    "    l = l.replace(\"]\",\"\")\n",
    "    l = l.split(\", \")\n",
    "    return l\n",
    "\n",
    "len(users)\n",
    "list(set(users[user[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatToString(f):\n",
    "    if not math.isnan(f):\n",
    "        return str(int(f))\n",
    "    else:\n",
    "        return \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the wiki names of the users\n",
    "usersWikis = {}\n",
    "print(len(activeDataSet.T.items()))\n",
    "for k, v in activeDataSet.T.items():\n",
    "    topUsers = makeStringToList(v[\"topUsers\"])\n",
    "    founder = floatToString(v['founding_user_id']\n",
    "    userWikiData = list(set(topUsers.append(founder)))\n",
    "    for user in userWikiData:                       \n",
    "        if user not in usersWikis and user != \"-1\":\n",
    "            if i % 500 == 0:\n",
    "                print(i, end=\", \")\n",
    "                usersWikis[user] = []\n",
    "            usersWikis[user].append(v['name'])\n",
    "            \n",
    "len(usersWikis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
