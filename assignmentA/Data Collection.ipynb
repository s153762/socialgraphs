{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import collections\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from fa2 import ForceAtlas2\n",
    "import numpy as np\n",
    "import datetime\n",
    "from lxml import html\n",
    "import glob\n",
    "# conda install -c phlya adjusttext \n",
    "#from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Get inital data #####\n",
    "n = 250\n",
    "\n",
    "# Initial query - get top wikis by pageviews using API\n",
    "query = \"http://www.wikia.com/api/v1/Wikis/List?expand=1&limit=\"+str(n)+\"&batch=1\"\n",
    "response = urlopen(query)\n",
    "wikisource = response.read()\n",
    "data = json.loads(wikisource)\n",
    "\n",
    "# Save data at data/wikis/top-wikis-date.json\n",
    "date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "with open(\"data/wikis/top-wikis-\"+date+\".json\", 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Read initial wiki data  ######\n",
    "df = pd.DataFrame()\n",
    "with open(\"data/wikis/top-wikis-\"+date+\".json\") as f:\n",
    "    json_file = json.loads(f.read())\n",
    "df = pd.DataFrame(json_file['items']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search for more wikis (primarily based on the different hub names)\n",
    "def saveSearchWikis(search, n):\n",
    "    querySearch = \"http://www.wikia.com/api/v1/Wikis/ByString?expand=1&string=\"+str(search)+\"&limit=\"+str(n)+\"&batch=1&includeDomain=true\"\n",
    "    response = urlopen(querySearch)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    if data['total'] > 0:\n",
    "        date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "        searchWikiPath = \"data/wikis/search-\"+search+\"-wikis-\"+date+\".json\"\n",
    "        with open(searchWikiPath, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "    else:\n",
    "        print(\"Data not found in %s: %s\" % (search, data['items']))\n",
    "    return pd.DataFrame(data['items']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 wikis found in Games\n",
      "250 wikis found in TV\n",
      "250 wikis found in Movies\n",
      "250 wikis found in Comics\n",
      "250 wikis found in Other\n",
      "250 wikis found in Books\n",
      "30 wikis found in Lifestyle\n"
     ]
    }
   ],
   "source": [
    "# use hubs in dataset to searsh for new wikis\n",
    "fullDataSet = pd.DataFrame()\n",
    "for hub in set(df[\"hub\"]):\n",
    "    df = saveSearchWikis(hub, n)\n",
    "    fullDataSet = fullDataSet.append(df)\n",
    "    print(\"%s wikis found in %s\" % (str(len(df)), hub))\n",
    "fullDataSet = fullDataSet.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search online to find the username based on the userID\n",
    "def saveUsernames(keyword):\n",
    "    keyword = str(keyword)\n",
    "    querySearch = \"http://community.wikia.com/wiki/Special:Search?search=\"+keyword+\"&fulltext=Search&ns2=1\"\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    usernames = re.findall(\"wiki\\/User:([(\\w)]+)?\",str(wikisource))\n",
    "    \n",
    "    # Read exsisting data to avoid duplicates\n",
    "    filename = \"data/users/userNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    \n",
    "    # Print new usernames\n",
    "    for user in set(usernames):\n",
    "        userData = user+\"\\n\"\n",
    "        if userData not in contents: # not a duplicate\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(userData)\n",
    "            fileWrite.close()\n",
    "\n",
    "    return list(set(usernames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search online to find the username based on the userID\n",
    "def saveUsernamesByUserID(userID):\n",
    "    \n",
    "    if userID == None:\n",
    "        print(\"user ID is %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    if int(userID) < 1:\n",
    "        print(\"user ID less than 1: %s\" % userID)\n",
    "        return []\n",
    "    \n",
    "    userID = str(userID)\n",
    "    querySearch = \"http://www.wikia.com/api/v1/User/Details?ids=\"+userID\n",
    "    response = urlopen(querySearch)\n",
    "    wikisource = response.read()\n",
    "    data = json.loads(wikisource)\n",
    "    \n",
    "    # Read exsisting data to avoid duplicates\n",
    "    filename = \"data/users/userNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    \n",
    "    usernames = []\n",
    "    # Print new usernames\n",
    "    for name in data['items']:\n",
    "        userData = name['title']+\" \"+name['user_id']+\"\\n\"\n",
    "        usernames.append(name['title'])\n",
    "        if userData not in contents: # not a duplicate\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(userData)\n",
    "            fileWrite.close()\n",
    "        elif name['title'] in contents:\n",
    "            content.replace(name['title'],userData)\n",
    "            fileWrite = open(filename,\"w\")\n",
    "            fileWrite.write(content)\n",
    "            fileWrite.close()\n",
    "\n",
    "    return usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search online to find the wiki names which the user use, \n",
    "# This wiki saves the users wikis and all the wiki names\n",
    "def saveNewWikisAndUsersThroughUser(usernames):\n",
    "    wikis = []\n",
    "    for username in usernames:\n",
    "        \n",
    "        # encode username correct\n",
    "        username = str(username.encode('utf-8'))\n",
    "        username = username.replace(\"'\", \"\")\n",
    "        username = username.replace(\"b\", \"\")\n",
    "        \n",
    "        # find users activity\n",
    "        querySearch = \"http://community.wikia.com/index.php?limit=1000&tagfilter=&title=Special%3AContributions&target=\"+username+\"&namespace=&tagfilter=&year=&month=-1.html\"\n",
    "        response = urlopen(querySearch)\n",
    "        wikisource = response.read()\n",
    "        \n",
    "        \n",
    "        # search html for wikis\n",
    "        usersWikis = re.findall(\"wiki/Adoption:(\\w+)?\",str(wikisource))\n",
    "        usersWikis = [wiki for wiki in usersWikis if (wiki != \"Requests\" and wiki != '')]\n",
    "        usersWikis = list(set(usersWikis))\n",
    "        \n",
    "        # Save users and the individual user's wikis \n",
    "        userData = username+\": \"+str(usersWikis)+\"\\n\"\n",
    "        \n",
    "        filename = \"data/users/user-wikis.txt\"\n",
    "        fileRead = open(filename, \"r\")\n",
    "        contents =fileRead.read()\n",
    "        \n",
    "        # Ensure no duplicates\n",
    "        if userData not in contents: # not a duplicate\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(userData)\n",
    "            fileWrite.close()\n",
    "        fileRead.close() \n",
    "        \n",
    "        \n",
    "        [wikis.append(wiki) for wiki in usersWikis]\n",
    "        wikis = list(set(wikis))\n",
    "        \n",
    "    # Save all found wiki names and read exsisting data to avoid duplicates\n",
    "    filename = \"data/wikis/wikiNames.txt\"\n",
    "    fileRead = open(filename, \"r\")\n",
    "    contents = fileRead.read()\n",
    "    fileRead.close() \n",
    "    for wiki in set(wikis):\n",
    "        wikiData = wiki+\"\\n\"\n",
    "        if wikiData not in contents:\n",
    "            # Print new usernames\n",
    "            fileWrite = open(filename,\"a\")\n",
    "            fileWrite.write(wikiData)\n",
    "            fileWrite.close()\n",
    "    \n",
    "    return wikis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collectAndSaveData(userID):\n",
    "    # Get and save all usernames\n",
    "    username = saveUsernamesByUserID(userID)\n",
    "    \n",
    "    # Get and save all wikis found connected to the usernames\n",
    "    wikiList = saveNewWikisAndUsersThroughUser(username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove inactive wikis and duplicates\n",
    "def findActiveWikis(dataSet):\n",
    "    activeDataSet = pd.DataFrame()\n",
    "    dataSet = dataSet.drop_duplicates(subset=\"id\")\n",
    "    # find stats in dataset\n",
    "    for s in dataSet['stats']:\n",
    "        # only add when at least 1 active\n",
    "        if s['activeUsers']>1:\n",
    "            data = dataSet.loc[dataSet['stats']==s]\n",
    "\n",
    "            activeDataSet = activeDataSet.append(data)#    \n",
    "    # remove duplicates\n",
    "    #activeDataSet = activeDataSet.reset_index(drop=True)\n",
    "    return activeDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data set has 35039 data, while only 5845 is actively used.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-58ae32bfd31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcollectAndSaveData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cce84bb307bf>\u001b[0m in \u001b[0;36mcollectAndSaveData\u001b[0;34m(userID)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollectAndSaveData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get and save all usernames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveUsernamesByUserID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get and save all wikis found connected to the usernames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-140a1f9b8727>\u001b[0m in \u001b[0;36msaveUsernamesByUserID\u001b[0;34m(userID)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Print new usernames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0muserData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0musernames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muserData\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# not a duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "# Get active wikis\n",
    "activeDataSet = findActiveWikis(fullDataSet)\n",
    "print(\"full data set has %s data, while only %s is actively used.\" % (len(fullDataSet),len(activeDataSet)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Find more users and their connected Wikis\n",
    "# find topusers\n",
    "topUsers = [x for x in activeDataSet[\"topUsers\"]]\n",
    "topUsers = list(set(itertools.chain.from_iterable(topUsers)))\n",
    "\n",
    "# find founders\n",
    "founders = [x for x in activeDataSet['founding_user_id']]\n",
    "\n",
    "users = list(set(topUsers + founders))\n",
    "\n",
    "for i in range(0,len(users)):\n",
    "    collectAndSaveData(users[i])\n",
    "    if i % 500 == 0:\n",
    "        print(i, len(users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2254\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35039"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Read Wiki Names ######\n",
    "# 22:49 - \n",
    "wikiDataPath = [f for f in glob.glob(\"data/wikis/*.json\")]\n",
    "print(len(wikiDataPath))\n",
    "i = 0\n",
    "for dataPath in wikiDataPath:\n",
    "    i+=1\n",
    "    with open(dataPath) as f:\n",
    "        json_file = json.loads(f.read())\n",
    "    fullDataSet = fullDataSet.append(pd.DataFrame(json_file['items']))\n",
    "    if(i%200 ==0):\n",
    "        print(i)\n",
    "\n",
    "fullDataSet = fullDataSet.drop_duplicates(subset=\"id\")    \n",
    "fullDataSet = fullDataSet.reset_index(drop=True) \n",
    "\n",
    "print(len(fullDataSet))\n",
    "fullDataSet = findActiveWikis(fullDataSet)\n",
    "print(len(fullDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Collected data\n",
    "date = datetime.date.today().strftime(\"%d-%m-%Y\")\n",
    "fullDataSet.to_csv(\"data/sortedWikiData-\"+date+\".csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not found in Adpotion_Request: []\n",
      "Data not found in Annunaki_Genesis_Wiki: []\n",
      "Data not found in Me_at_sp_in_: []\n",
      "Data not found in Tintin_Wiki_4: []\n",
      "Data not found in The_Miners_haven_project_Wikia: []\n",
      "Data not found in Cyber_Nantions_Wiki: []\n",
      "Data not found in Can_I_become_a_administrator_and_bureaucrat_on_this_wiki_communities: []\n",
      "Data not found in Cyberantions: []\n",
      "Data not found in Enter_wiki_name_here_: []\n",
      "Data not found in Brave_Frontier_Global_Wiki: []\n",
      "Data not found in Spleef_League_Wiki: []\n",
      "Data not found in I_made_a_new_account: []\n",
      "Data not found in Arabic_Steven_Universe_Wiki: []\n",
      "Data not found in TheThroneOfGlass_Wiki: []\n",
      "Data not found in Sakure_Hime_Kaden: []\n",
      "Data not found in Test_Drive_Wiki_: []\n",
      "100 af 2564\n",
      "Data not found in 3d_movies_wiki: []\n",
      "Data not found in Happy_Labs_Happy_Mall_Story_Wiki: []\n",
      "Data not found in Ugly_Betty_Wikia: []\n",
      "Data not found in Gruntipedia_: []\n",
      "Data not found in The_Throne_of_Glass_Wiki: []\n",
      "Data not found in Yuma100_Wiki: []\n",
      "Data not found in Kane_and_Lynch_Wiki: []\n",
      "Data not found in Drawn_To_Life_Wiki_: []\n",
      "Data not found in Test_Drive_Unlimited_Wikia: []\n",
      "Data not found in Shougeki_no_Soma_Fanon_Wiki: []\n",
      "Data not found in Alec_and_Ryan: []\n",
      "Data not found in Shoot_Em_Up_Wiki: []\n",
      "Data not found in Rytendo_Wiki: []\n",
      "200 af 2564\n",
      "Data not found in Mighty_Med_Wiki_: []\n",
      "Data not found in Haunted_Hathaways_Wiki_: []\n",
      "Data not found in Blonde_Sunrise_Wiki: []\n",
      "Data not found in The_Dork_Diaries_Wiki_: []\n",
      "Data not found in Fanmade_Gemmys_Wiki_: []\n",
      "Data not found in The_Encyclopedia_of_Rail_Transport_in_Hong_Kong: []\n",
      "300 af 2564\n",
      "Data not found in Kinichi_Wiki: []\n",
      "Data not found in Hanebado: []\n",
      "Data not found in NumbersWiki_: []\n",
      "Data not found in Shattered_Lands_Mod_Wiki: []\n",
      "Data not found in Trigun_Wiki_: []\n",
      "Data not found in Wiki_The_Sims_Fanon: []\n",
      "Data not found in Bartimaeus_Trilogy_Wiki: []\n",
      "Data not found in ModNation_Racers_Wiki_: []\n",
      "Data not found in Toy_Soldiers_Cold_War: []\n",
      "Data not found in Pixarplanes_Wiki: []\n",
      "Data not found in Disney_Princess_Wiki_2: []\n",
      "Data not found in Supernmariologan_wkiki: []\n",
      "Data not found in Gakkou_Garashi: []\n",
      "Data not found in Sumerian_Encyclopedia_Wiki: []\n",
      "400 af 2564\n",
      "Data not found in Dutch_Submachine_Wiki: []\n",
      "Data not found in One_Piece_Final_Chapter_Wiki: []\n",
      "Data not found in Sakure_Hime_Kaden_Wiki: []\n",
      "Data not found in The_Pink_Panther_Wiki_: []\n",
      "Data not found in Fairly_Odd_Fanon_Wiki_: []\n",
      "Data not found in T1E2H3_Wiki: []\n",
      "Data not found in Minecraft_Planet_Earth_Wiki: []\n",
      "Data not found in Lovelyz_Wiki: []\n",
      "Data not found in EA_UFC_Mobile_Wikia_: []\n",
      "Data not found in Militaryball_wiki: []\n",
      "Data not found in Stick_Empires_Fan_Stories_Wiki: []\n",
      "Data not found in French_Clarence_Wiki: []\n",
      "500 af 2564\n",
      "Data not found in The_Dating_Wiki: []\n",
      "Data not found in Spell_Book_Wiki: []\n",
      "Data not found in Big_Bang_Empire_Wikia: []\n",
      "Data not found in The_Irony_Lyons_Wiki: []\n",
      "Data not found in Alpha_and_Omega_Wiki_: []\n",
      "Data not found in Bananas_in_Pyjamas_Wiki_: []\n",
      "Data not found in Work_at_a_Pizza_Place_Wiki_: []\n",
      "Data not found in Creepypasta_Files_Wikia_: []\n",
      "Data not found in Dream_Kaleidoscope_Wiki_: []\n",
      "Data not found in Fan_Papa_Louie_Customers_Wiki: []\n",
      "600 af 2564\n",
      "Data not found in Fan_Papa_Louie_Customers: []\n",
      "Data not found in The_Final_Force_Wiki: []\n",
      "Data not found in Ace_Attorny_Fanon_Wiki: []\n",
      "Data not found in Larnojahokpedia_Wiki: []\n",
      "Data not found in Teen_Titan_Fan_Fiction_Wiki: []\n",
      "Data not found in Yahari_Wiki: []\n",
      "Data not found in Dresden_Files_Wiki_: []\n",
      "Data not found in Monster_Competitive_Wiki: []\n",
      "Data not found in The_Boss_Piggy_Wiki: []\n",
      "Data not found in WildStar_Online_Wikia: []\n",
      "700 af 2564\n",
      "Data not found in Mystical_Ninja_Wikia: []\n",
      "Data not found in El_Tigre_Wiki_: []\n",
      "Data not found in Crime_Coast_Wikia: []\n",
      "Data not found in The_Pretender_Wiki_: []\n",
      "Data not found in BEBONRPC_Wiki: []\n",
      "Data not found in Pokemon_Vortex_Wiki_: []\n",
      "Data not found in Oumagadoki_Zoo_Wiki_: []\n",
      "800 af 2564\n",
      "Data not found in Hap_Palmer_Wiki: []\n",
      "Data not found in The_Green_Ember_Wiki_: []\n",
      "Data not found in HillClimbRacingWiki: []\n",
      "Data not found in CBS_As_The_World_Turns_Wiki: []\n",
      "Data not found in Good_Operating_Systems_and_Apps_Wiki: []\n",
      "Data not found in JinjyaaFanFiki: []\n",
      "Data not found in FR_The_Vampire_Diaries_Wiki: []\n",
      "Data not found in I_Feel_Bad_Wiki: []\n",
      "Data not found in Gakuen_Babysister_Wiki: []\n",
      "Data not found in All_Dogs_go_to_Heaven_Wiki_: []\n",
      "900 af 2564\n",
      "Data not found in Floricienta_Wiki_spanish: []\n",
      "Data not found in Animal_Crossing_lets_hate_this_game_wiki: []\n",
      "Data not found in Newer_Super_Mario_Bros_Wiki: []\n",
      "Data not found in Polish_Sonic_Wiki: []\n",
      "Data not found in Xenosaga_Wiki_: []\n",
      "Data not found in ONE_PUNCH_MAN_ITALIAN_WIKI: []\n",
      "1000 af 2564\n",
      "Data not found in Spanish_Danganronpa_wiki: []\n",
      "Data not found in Sweet_Dreams_Wiki: []\n",
      "Data not found in The_Mighty_B_Wiki: []\n",
      "Data not found in Svenska_Fallout_Wiki: []\n",
      "Data not found in Nancy_Drew_Wikia: []\n",
      "Data not found in Hearts_Of_Medicine_Wika: []\n",
      "Data not found in Unlimited_Ninja_Wiki_: []\n",
      "Data not found in Polandball_Fandom_Wiki: []\n",
      "1100 af 2564\n",
      "Data not found in The_United_State_of_America_wiki: []\n",
      "Data not found in Turkey_Outlast_Wiki: []\n",
      "Data not found in Horrible_Songs_and_Music_Wiki: []\n",
      "Data not found in Turkey_Pokemon_Wikia: []\n",
      "Data not found in The_Mew_Project: []\n",
      "Data not found in Animal_FanFiction_Wiki: []\n",
      "Data not found in Hanasaku_Iroha_Wiki_: []\n",
      "Data not found in MyAnimeList_Community_Wiki: []\n",
      "Data not found in Shinobi_Life_O_A_Wiki: []\n",
      "Data not found in Wiki_Minaj_: []\n",
      "Data not found in Disney_Princess_Wiki_3: []\n",
      "Data not found in Mamamoo_Wiki: []\n",
      "Data not found in Dream_Theater_Wiki_: []\n",
      "1200 af 2564\n",
      "Data not found in Hanebado_Wiki: []\n",
      "Data not found in Dangan_Ronpa_Fanon_Wiki_: []\n",
      "Data not found in Dungeon_survivor_2: []\n",
      "Data not found in Radiohead_Knowledge_Base_: []\n",
      "Data not found in Kalle_Anke_Sverige_Wiki: []\n",
      "Data not found in Phoenix_Files_Wiki_: []\n",
      "Data not found in Kebler: []\n",
      "Data not found in Star_vs_the_Forces_of_Evil_Ships_Wika: []\n",
      "Data not found in Inspector_Gadget_Wiki_2: []\n",
      "1300 af 2564\n",
      "Data not found in Prison_Architect_Fandom: []\n",
      "Data not found in PetAdoptables_Wiki: []\n",
      "Data not found in Ultima_Codex_Wiki: []\n",
      "Data not found in Vampire_Knight_Enciclop: []\n",
      "Data not found in Dog_Wikii: []\n",
      "Data not found in Tony_Hawk_Games_Wiki: []\n",
      "Data not found in Justified_Wikia: []\n",
      "Data not found in Steam_Trading_Cards_Wiki_: []\n",
      "1400 af 2564\n",
      "Data not found in Heiko_401_Survivor_Wikia: []\n",
      "Data not found in MeiQ_Wiki: []\n",
      "Data not found in Smallerontheoutside_Wiki: []\n",
      "Data not found in Lemony_Snicket_Wiki_: []\n",
      "Data not found in Yumerio_Patissiere_Wiki: []\n",
      "Data not found in CSI_Cyber_Wiki: []\n",
      "Data not found in Qubo_Miss_BG_Wiki: []\n",
      "Data not found in BoBoiBoy_Wiki_Indonesia_: []\n",
      "Data not found in Legoapedia: []\n",
      "Data not found in Joy_Ride_Wiki: []\n",
      "Data not found in Minecraft_Wiki_Indonesia: []\n",
      "1500 af 2564\n",
      "Data not found in Super_Mario_64_Hack_Wiki: []\n",
      "Data not found in Project_Cars_Wiki: []\n",
      "Data not found in When_in_Rome_Wiki: []\n",
      "Data not found in MightyMedFannon_Wiki: []\n",
      "Data not found in Dylan_Kart_Wikia: []\n",
      "Data not found in Lord_of_Arcana_Wiki_: []\n",
      "Data not found in Death_Eater_United_Roleplay_Wiki: []\n",
      "Data not found in Dungeonworld_Wiki: []\n",
      "1600 af 2564\n",
      "Data not found in Little_Einsteinspedia_Wiki: []\n",
      "Data not found in Partials_Wiki_: []\n",
      "Data not found in Red_Faction_Wikia: []\n",
      "Data not found in Babywearing_Wiki: []\n",
      "Data not found in Supremacy_1914_Wiki: []\n",
      "Data not found in California_Dreaming_Wiki_: []\n",
      "Data not found in Star_Wars_wikis: []\n",
      "Data not found in Eminem_Wiki_2: []\n",
      "Data not found in Yoru_no_Nai_Kuni_Wikia: []\n",
      "Data not found in Stitchipedia: []\n",
      "Data not found in Applemasterexpert: []\n",
      "1700 af 2564\n",
      "Data not found in Despicable_Me_Wiki_: []\n",
      "Data not found in ROBLOX_Disney_Parks_Wiki: []\n",
      "Data not found in Sonic_core_Control_wiki: []\n",
      "Data not found in TF2_Freak_Concept_Wiki_: []\n",
      "Data not found in Alice_in_Wonderland_Wiki_: []\n",
      "Data not found in The_Orville_Wiki_: []\n",
      "Data not found in Zoopedia_2_Wikk: []\n",
      "Data not found in Macne_Series_Wiki_: []\n",
      "Data not found in Bible_Black_Wiki: []\n",
      "Data not found in All_Victoria: []\n",
      "Data not found in Diary_of_a_Wimpy_Kid_Fanon_Wiki_: []\n",
      "1800 af 2564\n",
      "Data not found in Tetris_Battle_Wiki: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not found in Warriors_Answers_Wiki: []\n",
      "Data not found in Mad_Cartoon_Network_Wiki_: []\n",
      "Data not found in SoNyuhShiDae_Wiki_: []\n",
      "Data not found in Encyclopedia_Scumdoggia_Wiki_: []\n",
      "Data not found in Singapore_Transport: []\n",
      "Data not found in Macne_Series_wikia: []\n",
      "Data not found in Encyclopeadia_Scumdoggia_Wiki_: []\n",
      "Data not found in Far_Cry_Wiki_2: []\n",
      "Data not found in Rhett_and_Link_Wiki_: []\n",
      "Data not found in Bizaardvark_Wiki_: []\n",
      "Data not found in Fluffypedia_Wiki_3: []\n",
      "Data not found in Prisoner_Cell_Block_H_Wiki_: []\n",
      "Data not found in Singapore_Transport_Wiki_: []\n",
      "1900 af 2564\n",
      "Data not found in Crash_and_Fluttershy_Fanon_Wiki: []\n",
      "Data not found in Dancedancerevolutionddr: []\n",
      "Data not found in Spider_Riders_Wiki: []\n",
      "Data not found in Czech_Naruto_Wiki: []\n",
      "Data not found in Spider_Riders_Center: []\n",
      "Data not found in The_Magic_School_Bus_wiki_: []\n",
      "Data not found in RemiseTale_Wiki: []\n",
      "Data not found in MOGAI_Encyclopedia_Wikia: []\n",
      "Data not found in Encyclopaedia_Scumdoggia_Wiki_: []\n",
      "Data not found in Prisoner_Cell_Block_H_Wikia: []\n",
      "Data not found in Star_Ocean_Wiki_: []\n",
      "Data not found in Lego_Agents_Wiki_: []\n",
      "Data not found in Chuggi_Wiki: []\n",
      "Data not found in Steven_Universe_Wiki_2: []\n",
      "Data not found in GTA_Wiki_Vietnamese: []\n",
      "Data not found in Inazuma_Eleven_Shipping_Wiki_2: []\n",
      "Data not found in Blood_and_Ice_Cream_Wiki: []\n",
      "Data not found in Bureaucrat_rights_request: []\n",
      "2000 af 2564\n",
      "Data not found in Welcome_to_the_Trollz_World_Wiki_: []\n",
      "Data not found in Kick_Buttowski_Wiki_: []\n",
      "Data not found in Requesting_for_Bureaucrat_Status_on_Fatal: []\n",
      "Data not found in Welcome_to_the_Trollz_World_Wiki_Re_Adopt: []\n",
      "Data not found in Grain_and_Mango: []\n",
      "Data not found in Mikaze_Ai_wiki: []\n",
      "Data not found in CoC_Revamp_Mod_Wikia: []\n",
      "Data not found in Whose_Line_Is_It_Anyway_Wiki_: []\n",
      "Data not found in Looney_Tunes_Comics_Wiki_: []\n",
      "Data not found in Platinum_Games_Wiki_: []\n",
      "Data not found in BENDROWNEDYOURTURN_wiki: []\n",
      "Data not found in Wikia_Seikimatsu_Occult_Gakuin_Wiki: []\n",
      "Data not found in Work_and_Glory_Wiki: []\n",
      "Data not found in Doctor_Who_Expanded_: []\n",
      "2100 af 2564\n",
      "Data not found in Casper_the_Friendly_Ghost_Wiki_: []\n",
      "Data not found in Type_Racer_Wiki: []\n",
      "Data not found in Hoshin_Engi_Wiki: []\n",
      "Data not found in MinecraftCreepypasta2_Wiki: []\n",
      "Data not found in Rejected_Trollpasta_Wikia: []\n",
      "Data not found in LittleBigFanon_Wiki_: []\n",
      "Data not found in Czech_Pok: []\n",
      "Data not found in NarutoTR_Wiki: []\n",
      "Data not found in Harry_Potter_Answers_: []\n",
      "Data not found in Hitchhikers_Wiki_: []\n",
      "2200 af 2564\n",
      "Data not found in Minecraft_mob_Wikia: []\n",
      "Data not found in Corpse_Bride_Wiki_: []\n",
      "Data not found in WikiFAYZ_: []\n",
      "Data not found in PT_Nerf_Wiki: []\n",
      "Data not found in Toxic_Fandoms_and_Hatedoms_Wikia: []\n",
      "Data not found in Star_Butterfly_Wiki: []\n",
      "Data not found in Papa_Louie_Fanon_Wiki_: []\n",
      "Data not found in Fifty_Shades_of_Grey_Wiki_: []\n",
      "Data not found in Kokoro_Connect_Wiki_: []\n",
      "Data not found in Hello_Charlotte_Wiki_: []\n",
      "2300 af 2564\n",
      "Data not found in Infinite_Flight_Wiki_2: []\n",
      "Data not found in The_CHERUB_Wikia: []\n",
      "Data not found in Little_Bear_Wiki_: []\n",
      "Data not found in The_Disney_Princess_Wiki: []\n",
      "Data not found in Emulation_General_Wikia: []\n",
      "Data not found in My_Tribe_for_PC_Wikia: []\n",
      "Data not found in IEP_Fanon_Wiki: []\n",
      "Data not found in Domestic_Violence_Wiki_: []\n",
      "Data not found in The_Asterisk_War_Fanon_Wiki: []\n",
      "Data not found in Historical_boat_Wiki: []\n",
      "Data not found in Isekai_Fanon_Wiki: []\n",
      "Data not found in Naruto_x_Boruto_Ninja_Voltage_Wiki: []\n",
      "Data not found in Dead_Space_Fanon_Wiki_: []\n",
      "Data not found in Disney_Princess_Wiki_4: []\n",
      "2400 af 2564\n",
      "Data not found in The_Lost_Room_Wiki_: []\n",
      "Data not found in The_2nd_Oggy_and_the_Cockroaches_Fanon_Wiki: []\n",
      "Data not found in Holly_Hobbie_and_Friends_Wiki: []\n",
      "Data not found in Harry_Potter_Answers_Wiki: []\n",
      "Data not found in Burn_Notice_wiki: []\n",
      "Data not found in Crappy_Games_Wikia: []\n",
      "Data not found in Crank_Yankers_Wiki_: []\n",
      "Data not found in Alphablock_Wiki_: []\n",
      "Data not found in Requests_Adoption_Guidelines: []\n",
      "Data not found in Fresh_Prince_Wiki_: []\n",
      "Data not found in Regular_Car_Wiki: []\n",
      "Data not found in Bureaucrat_On_Ponyo_Wiki: []\n",
      "Data not found in Regular_Cars_Wiki: []\n",
      "Data not found in Good_Luck_Charlie_Wiki_: []\n",
      "2500 af 2564\n",
      "Data not found in The_Polar_Express_Wiki_: []\n",
      "Data not found in Video_game_boss_wiki: []\n",
      "Data not found in Krypto_the_Superdog_Wiki_: []\n",
      "Data not found in Stuart_Little_Wiki_: []\n",
      "Data not found in Loco_Roco_Wiki_: []\n",
      "Data not found in Youkoso_Jitsuryoku_Shijou_Shugi_no_Kyoushitsu_e_Wiki: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31479"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### add Wiki Names ######\n",
    "# 12.21\n",
    "# Read exsisting data to avoid duplicates\n",
    "wikiNames = []\n",
    "filename = \"data/wikis/wikiNames.txt\"\n",
    "f = open(filename, \"r\")\n",
    "for line in f.readlines():\n",
    "    wikiNames.append(line.replace(\"\\n\",\"\"))\n",
    "f.close() \n",
    "\n",
    "i = 0\n",
    "for wikiName in wikiNames:\n",
    "    i+=1\n",
    "    df = saveSearchWikis(wikiName, n)\n",
    "    fullDataSet = fullDataSet.append(df)\n",
    "    if i%100==0:\n",
    "        print(\"%i af %s\" % (i,len(wikiNames)))\n",
    "\n",
    "fullDataSet = fullDataSet.drop_duplicates(subset=\"name\")    \n",
    "fullDataSet = fullDataSet.reset_index(drop=True)    \n",
    "len(fullDataSet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1111), (22224, 1093), (957747, 1087), (36156286, 774), (5275700, 616), (4784321, 562), (29325840, 459)]\n",
      "35983\n"
     ]
    }
   ],
   "source": [
    "users = [x for x in fullDataSet[\"topUsers\"]]\n",
    "users.append([x for x in activeDataSet['founding_user_id']])\n",
    "              \n",
    "users = list(itertools.chain.from_iterable(users))\n",
    "\n",
    "counter=collections.Counter(users)\n",
    "#print(counter)\n",
    "#print(counter.values())\n",
    "#print(counter.keys())\n",
    "print(counter.most_common(7))\n",
    "\n",
    "print(len(set(users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the wiki names of the users\n",
    "userWiki = {}\n",
    "for user in list(set(users)):\n",
    "    for k, v in fullDataSet.T.items():\n",
    "        if user == v[\"topUsers\"] or user == v['founding_user_id']:\n",
    "            if user not in userWiki:\n",
    "                userWiki[user] = []\n",
    "            userWiki[user].append(v['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
